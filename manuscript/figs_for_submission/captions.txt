Fig. 1

A simple probabilistic graphical (Bayesian) model we will validate in this work. (a) When read from top to bottom, the graphical model describes a generative process (see the legend for the meaning of vertical lines and downward-pointing arrows). If read from bottom to top, the graphical model describes the process of inference (assuming arrows having opposite orientation denoting the flow of information); in this case, the blue and yellow circles represent the data and the parameters being estimated, respectively. A random variable within a rectangular box signifies a parameter whose value is assumed known by the user; these are normally nuisance hyperparameters, or parameters that are not of immediate interest perhaps because they have been estimated elsewhere. (b) Each random variable node in the model, and how they should be interpreted. Table 1 presents more detail on each of the sampling distributions. Briefly, ``LN'' stands for log-normal, ``Yule'' for a Yule process also known as a pure-birth model, and ``PhyloBM''  stands for a phylogenetic Brownian motion model.

Fig. 2

Validation results for Yule model simulator. (a) How often the expected Yule-tree root age fell within its 95%-confidence intervals, for different birth rates (see main text for more details). (b) Graphical representation of table in (a).

Fig. 3

Flowchart of the validation of a Bayesian model. Standard flowchart symbols are explained in the legend. The flowchart area with a clear background is where (true) parameters and data are generated, and where the model simulator(s) is validated. The flowchart area shaded in pink mark the steps involved in validating the inference engine once the data has been generated. $\theta$ denotes a vector with $n$ elements, where each element is an i.i.d. parameter(s) sample from its (their) corresponding prior(s) $f_\Theta(\cdot)$. Analogously, $\boldsymbol{d}$ denotes a vector with $n$ elements, where each element is an i.i.d. data sample from the corresponding likelihood(s) $f_{D|\Theta}(\cdot|\Theta\mathop{=}\theta)$. $\boldsymbol{\theta}'$ holds $n \times L$ elements, with each being one of the $L$ posterior samples for each of the $n$ parameter samples in $\theta$. All $L$ posterior samples obtained from the $i$-th data set $d_i$ comprise together what one would call the posterior distribution over $\theta_i$. Posterior samples are commonly obtained through MCMC. $\text{U}(l, u)$ denotes a uniform distribution with and including lower and upper bounds $l$ and $u$, respectively. The acquamarine dotted box encloses the stages of the pipeline that are exclusive to the coverage validation procedure. The pink dotted box encloses the stages of the pipeline that are exclusive to the rank-uniformity validation (RUV) procedure.

Fig. 4

Coverage validation analyses of the Bayesian hierarchical model in Fig. 1. Panels show the true (i.e., simulated) parameter values plotted against their mean posteriors (the dashed line shows $x = y$). Dots and lines (100 per panel) represent true values and their 95\%-HPDs, respectively. Simulations for which 95\%-HPDs contained the true value are highlighted in blue, otherwise are presented in red. (a) In ``Scenario 1'', the model used in inference was the least missspecified (low levels of misspecification were introduced by rejection sampling, when one in ten trees were rejected). (b) In ``Scenario 2'', the model used in inference was misspecified beyond the effect of rejection sampling (which was the only source of misspecification in ``Scenario 1'' and ``Scenario 3''); here, we used the same data sets simulated in ``Scenario 1''. (c) In ``Scenario 3'' the model was missspecified as a result of rejection sampling as in ``Scenario 1'', with the difference that a greater proportion of trees were rejected (approximately 90\% of trees were rejected, with only those having between 100 to 200 tips being kept).

Fig 5

Patterns observable after inference in rank-uniformity validation (RUV). We explain how to interpret the histogram of ranks (middle column) and ECDF plots (right-hand side column) in the main text. (a) Model implementation is correct. (b) Parameter estimates are overdispersed relative to their true values. (c) Parameter estimates are underdispersed relative to their true values. (d) Parameter estimates are consistently overestimated relative to their true values. In the left-hand side column, the prior and replicate-averaged posterior (also known as the data-averaged posterior) distributions over some parameter $\theta$ are shown in light blue and dark blue, respectively. In the middle graphs, light-blue bands represent the 95\%-confidence interval about the expected rank count, and horizontal black lines mark the rank count mean. Light-blue ellipses in the rightmost graphs represent confidence intervals about the empirical cumulative distribution function (ECDF).

Fig 6

Rank-uniformity validation (RUV) of the Bayesian hierarchical model in Fig. 1. Panels in the top row show the histograms of $n=100$ ranks, for parameter $R$ in each scenario, obtained after 10\% burnin and thinning of posterior samples down to 200 out of 10,000. Panels in the bottom row show the corresponding ECDF plots, for parameter $R$ in each scenario. (a) In ``Scenario 1'', the inference model was the least miss specified (low levels of misspecification were introduced by rejection sampling) and we can see that the ranks are compatible with a uniform distribution (within the blue band). 
 (b) In ``Scenario 2'', the inference machinery was misspecified beyond the effect of rejection sampling (which was the only source of misspecification in ``Scenario 1'' and ``Scenario 3''); here, we used the same data sets simulated in ``Scenario 1''. A clear pattern of overestimation shows up in the ranks, meaning the ranks for the data-generating values are usually smaller than expected under correctness. (c) In ``Scenario 3'' we can see a pattern of underestimation, evidenced by ranks bunching up to the right. Rejection sampling was more extreme in this scenario (i.e., a more misspecified model in inference) than in that shown in (a).

Fig 7

Coverage validation and rank-uniformity validation (RUV) of the Bayesian hierarchical model in Fig. 1, for each scenario described in the main text (also see Figs. 4 and 6), with respect to the height of $\phi$ (i.e., the phylogeny's root age). Panels in the top row show the true (i.e., simulated) root age values plotted against their mean posteriors (the dashed line shows $x = y$). Dots and lines (100 per panel) represent true values and their 95\%-HPDs, respectively. Simulations for which 95\%-HPDs contained the true value are highlighted in blue, otherwise are presented in red. Panels in the middle row show the RUV histograms of $n=100$ ranks in each scenario, obtained after 10\% burnin and thinning of posterior samples down to 200 out of 10,000. Panels in the bottom row show the corresponding RUV ECDF plots in each scenario.

Fig 8

Coverage validation and rank-uniformity validation (RUV) of a Kingman's coalescent model with respect to the Robinson-Foulds distance (RF; see text in box) between the coalescent tree ($\Phi$) and a reference (random) tree ($\phi_0$). The effective population size parameter is assumed known and fixed to 1.0 during inference. (a) The true RF distances (i.e., between 100 simulated coalescent trees, $\boldsymbol{\phi} = \{\phi_i: 1 \leq i \leq 100\}$, and the same reference tree, $\phi_0$) plotted against the corresponding mean posterior RF distances (calculated from posterior samples of each $\phi_i$ an $\phi_0$). The dashed line shows $x = y$. Dots and vertical lines represent true RF-distance values and their estimated 95\%-HPDs, respectively. Simulations for which 95\%-HPDs contained the true value are highlighted in blue, otherwise are presented in red. (b) RUV histograms of 100 ranks obtained after 10\% burnin and thinning of posterior samples down to 1,000 out of 10,000. (c) RUV empirical CDF plots.




