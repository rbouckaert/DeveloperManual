\documentclass[oneside]{article}

\usepackage{blindtext} % dummy text throughout this template 
\usepackage{graphicx} % figures
\usepackage{wrapfig}
\usepackage{color} % for colored text
\usepackage{colortbl} % for colored text
\usepackage{float} % for forcing Figure placement
\usepackage[breakable]{tcolorbox} % text box
\usepackage{enumerate} % bullet point lists
\usepackage{setspace} % line spacing
\usepackage{soul} % strike through
\usepackage[normalem]{ulem} % strike through keeping emphasis standard
\usepackage{cancel} % diagonal strike through
\usepackage[margin=1in]{geometry} % margins
\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
% \linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aestheticsgins
\usepackage[small,labelfont=bf,up,up]{caption} % Custom captions under/above floats in tables or Figures
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{multirow}
\usepackage{amsmath} % Text in equations
\usepackage{titlesec} % Allows customization of titles
\usepackage{titling} % Customizing the title section
\usepackage[hidelinks]{hyperref} % For hyperlinks in the PDF
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{lineno} % for line numbering

\usepackage{tikz}
\usepackage{bm}
\usepackage{scalefnt}
\usetikzlibrary{shapes.geometric, arrows, positioning, decorations.markings, shapes.multipart}
\usetikzlibrary{bayesnet}
\usepackage{standalone}

\usepackage{xr} % cross-referencing files
\externaldocument{method_validation_ms_supp}

\usepackage{subcaption} % for subfigures side by side
\captionsetup[subfigure]{singlelinecheck=false} % to put (a) and (b) at the top-left of subfigures
\usepackage[shortlabels]{enumitem} % customized lists (shortlabels
                                % necessary to have i., ii., etc., in enumerate)
\setlist[itemize]{noitemsep} % Make itemize lists more compact

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text
\usepackage{colortbl} % grey stuff in the tables
\definecolor{LightGrey}{gray}{0.9}

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{Mendes et al. $\bullet$ February 2024} %$\bullet$ bio{\color{red}R}$\chi$ve} % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text



\usepackage{libertine}

\setlength\columnsep{20pt}



%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

%\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
%\posttitle{\end{center}} % Article title closing formatting

\pagestyle{fancy}
\fancyhead[R]{}
\fancyhead[L]{}
\chead{BAYESIAN EVOLUTIONARY MODEL VALIDATION}

\title{How to validate a Bayesian evolutionary model} % Article title
%LM: strictly speaking, we're 'validating' the inference machinery.
%% LM: Validating a model involves epistemic considerations that are, as far as I understand, outside the scope of this paper.
\author{\textsc{F\'{a}bio K. Mendes$^{1\dagger*}$}, \textsc{Remco Bouckaert$^{2\dagger}$},\\
\textsc{Luiz M. Carvalho$^{3\dagger}$}, \textsc{Alexei J. Drummond$^{4}$} \\
\small $^1$Department of Biology, Louisiana State University, Baton Rouge, LA 70803, United States\\
\small $^2$School of Computer Science, The University of Auckland, Auckland 1010, New Zealand\\
\small $^3$Escola de Matem\'{a}tica Aplicada, Funda√ß\~{a}o Getulio Vargas, Rio de Janeiro, RJ 22250-900, Brazil\\
\small $^4$School of Biological Sciences, The University of Auckland, Auckland 1010, New Zealand\\
\small
\href{mailto:fmendes@lsu.edu}{$^*$Corresponding author: fmendes@lsu.edu}\\
{\small $^\dagger$Authors contributed equally to this work}
%\and % Uncomment if 2 authors are required, duplicate these 4 lines if more
%\textsc{Jane Smith}\thanks{Corresponding author} \\[1ex] % Second author's name
%\normalsize University of Utah \\ % Second author's institution
%\normalsize \href{mailto:jane@smith.com}{jane@smith.com} % Second author's email address
}
\date{\today} % Leave empty to omit a date
\renewcommand{\maketitlehookd}{%
\begin{abstract}
  \noindent Biology has become a highly mathematical discipline in which probabilistic models play a central role.
  As a result, research in the biological sciences is now dependent on computational tools capable of carrying out complex analyses.
  These tools must be validated before they can be used, but what is
  understood as validation varies widely among methodological contributions.
  This may be a consequence of the still embryonic stage of the literature on statistical software validation for computational biology.
  Our manuscript aims to advance this literature.
  Here, we describe, illustrate and introduce new good practices for assessing the correctness of a model implementation, with an emphasis on Bayesian methods.
  We also introduce a suite of functionalities for automating validation protocols.
  It is our hope that the guidelines presented here help sharpen the focus of discussions on (as well as elevate) expected standards of statistical software for biology.
\end{abstract}
\centering [Probabilistic model, Bayesian model, model validation, coverage]
}

%----------------------------------------------------------------------------------------

\doublespacing

\linenumbers

\begin{document}

% Print the title
\maketitle

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}
The last two decades have seen the biological sciences undergo a major revolution.
Critical technological innovations such as the advent of massive parallel sequencing and the accompanying improvements in computational power and storage have flooded biology with unprecedented amounts of data ripe for analysis.
Not only has intraspecific data from multiple individuals allowed progress in fields like medicine and epidemiology \citep[e.g.,][]{1000g,humanmicrobiome,neafsey15}, population genetics \citep[e.g.,][]{lynch07,lack16,demanuel16} and disease ecology \citep[e.g.,][]{rosenblum13,bates18}, but now a large number of species across the tree of life have had their genomes sequenced, furthering our understanding of species relationships and diversification \citep[e.g.,][]{pease2016,kawahara19,upham19}.
Almost on par with \textcolor{red}{\st{with}} data accumulation is the rate at which new computational tools are being proposed, as evidenced by journals entirely dedicated to method advances, methodological sections in biological journals, and computational biology degrees being offered by institutions around the world.

One extreme case is the discipline of evolutionary biology, on which we focus our attention.
While it could be said that many decade-old questions and hypotheses in evolutionary biology have aged well and stood up the test of time (e.g., the Red Queen hypothesis, \citealt{vanvalen73,lively87,morran11,gibson15}; the Bateson-Dobzhansky-Muller model, \citealt{dob36,muller40,hopkins12,roda17}),
data analysis practices have changed drastically in recent years, to the point they would likely seem exotic and obscure to an evolutionary biologist active forty years ago. 
In particular, evolutionary biology has become highly statistical, with the development and utilization of probabilistic models now being commonplace.

Models are employed in the sciences for many reasons, and fall within a biological abstraction continuum \citep{servedio14}, going from fully verbal, highly abstract models (e.g., \citealt{vanvalen73}), through proof-of-concept models that formalize verbal models (e.g., \citealt{maynard78,reinhold99}), to models that interact directly with data through explicit mathematical functions \citep{yule24,fel73,hky,hudson90}.
Within the latter category, probabilistic models have seen a sharp surge in popularity within evolutionary biology, in conjunction with computational tools implementing them.

Despite the increasing pervasiveness of probabilistic models in the biological sciences, tools implementing such models show large variation not only with respect to code quality (from a software engineering perspective), but also to the provided evidence for correctness \citep{darriba18}.
This is unsurprising given the challenges in funding software research \citep{siepel19}, and the multidisciplinary nature of method development.
Much of the relevant information regarding good coding and statistical practices is out of reach of the average computational biologist, as it is spread across a variety of specialized sources, often obfuscated by its technical and theoretical presentation.
The bioinformatics community is thus in dire need of synthetic and accessible resources that provide guidance for code improvement and validation.

Here, we summarize best practices in probabilistic model validation for method developers, with an emphasis on Bayesian methods.
We execute two different validation protocols on variations of a simple phylogenetic model, discuss the results, and expand on how to interpret other potential outcomes.
We further introduce a suite of methods for automating these protocols within the BEAST 2 platform \citep{beast25}.
Lastly, we propose method development guidelines for new model contributions, for researchers and reviewers who expect new software to meet not only a desirable standard, but also a reasonable one.

%------------------------------------------------

\section*{Probabilistic models}
\label{sec:prob_models}

Probabilistic models mathematically formalize natural phenomena
having an element of randomness.
This is done through probability distributions describing both the observed
empirical data -- seen as the result of one or more random instantiations of the modeled process -- as well the model parameters, which abstract relevant, but usually unknown aspects of the phenomenon at hand.
In the domain of evolutionary biology specifically, the historical, stochastic, and highly dimensional nature of evolutionary processes makes the utility of probabilistic models self-evident. 

The central component of a probabilistic model, $\text{Pr}(D=d|\Theta=\theta)$, allows us to describe the probability distribution over the data \textcolor{red}{\st{($D$)}}\textcolor{blue}{$D$ (which takes value $d$)} given the model parameters ($\Theta$) (which take values $\theta$).
This probability mass function (pmf; or its continuous counterpart, the probability density function, pdf, $f_D(d|\Theta=\theta)$) is sometimes referred to as the likelihood function.
Just for this section, we will abuse and simplify \textcolor{blue}{the} notation \textcolor{blue}{for the image of $f_D$}, and drop variable subscripts, e.g., we will write $f_D(d|\Theta=\theta)$ as $f(d|\theta)$.
As illustrated in the next sections, probabilistic models can be hierarchical, in which case there may be several likelihood functions.
In a frequentist statistical framework, $f(d|\boldsymbol{\theta})$ is the sole component of an inferential procedure, and is maximized across parameter space during parameter estimation and model comparison.

In the present study we focus on Bayesian inference, where a probabilistic model $\mathcal{M}$ defines a posterior  probability distribution for its parameters, $f(\theta|d) = \frac{f(d|\theta)f(\theta)}{f(d)}$.
Here, our prior inferences or beliefs about the natural world -- represented by the prior distribution $f(\theta)$ -- are confronted with and updated by the data through the likelihood function.
Crucially, a Bayesian model includes a prior, $f(\theta)$: when models are compared, for example, $f(\theta)$ needs to be taken into account when computing the model evidence $f(d)$.

Models routinely used in evolutionary biology are often characterized by continuous parameters, and are normally complex enough to preclude analytical solutions for the posterior density $f(\theta|d)$, mainly due to the intractability of the integral appearing in the denominator -- i.e., the marginal likelihood.
In those cases, one can make use of the fact that $f(d)$ is a constant with respect to the parameters that can be ignored (i.e., $f(\theta|d) \propto f(\theta|d)f(\theta)$), and use techniques like Markov chain Monte Carlo (MCMC) to sample \textcolor{blue}{(and hopefully converge on)} the posterior distribution.
This is because MCMC is usually implemented in the form of the Metropolis-Hastings \citep{metropolis53,mh} algorithm, which only requires the posterior to be evaluated up to a constant.

In practice, the Metropolis-Hastings algorithm samples the posterior distribution (also referred to as the ``target'' distribution) by means of a transition mechanism \textcolor{blue}{i.e., a set of proposal functions}. 
If the proposal distribution generated by this mechanism \textcolor{blue}{produces a Markov chain that }is \textcolor{blue}{(i) }irreducible \textcolor{blue}{(any ``state'', or combination of parameter values, can be eventually reached from any other state)}, \textcolor{blue}{(ii) positive recurrent (there is an expected finite time for a state to be returned to)}, and \textcolor{blue}{(iii) }aperiodic \textcolor{blue}{(every state has a period of 1, a requirement for most initial distributions to converge on the posterior distribution)}, and the chain is long enough, then the sampled posterior distribution will closely approximate the target distribution $f(\theta|d)$ \textcolor{blue}{(}\citealp{smith93,tierney94,gelman}\textcolor{blue}{; we point interested readers to those references for more formal definitions)}.

We will spend time considering MCMC in particular, as it is the commonly chosen technique for obtaining samples from $f(\theta|d)$ under an implementation of model $\mathcal{M}$.
A thorough validation effort thus entails verifying the correctness of (i) the model (i.e., $f(d|\theta)f(\theta)$), and (ii) the components involved in the MCMC transition mechanism.
We note that the latter are not part of the model, however, and it is  possible to sample $f(\theta|d)$ with other techniques such as importance sampling, Laplace approximations \citep{inla}, or even by converting the sampling problem into an optimization one \citep[e.g.,][]{zhang18} \textcolor{blue}{and variational inference \citep[e.g.,][]{ zhang2022variational,bouckaert2024variational}}.

Finally, we stress that we are interested in practices for verifying model correctness \textcolor{blue}{-- and by ``correctness'', we mean the correctness of a model implementation}.
There are other tests and diagnostics employed to ensure that a particular MCMC analysis is converging as expected.
Ascertaining whether one or more independent Markov chains have converged to a given posterior distribution is not a correctness test, as that distribution might be very different from the target distribution. 
We refer the reader interested in these and related topics to \cite{rwty}, \cite{fabreti2022}, \cite{magee2023} and references therein.

\section*{Validating a Bayesian model}

In this section we discuss procedures for validating an implementation of a Bayesian model $\mathcal{M}$.
Whenever necessary, we will differentiate between a model implemented as a simulator ($\text{S}[\mathcal{M}]$) and as a tool for inference ($\text{I}[\mathcal{M}]$).
Both $\text{S}[\mathcal{M}]$ and $\text{I}[\mathcal{M}]$ must be inspected in order to validate a model $\mathcal{M}$.

\subsection*{Validating the simulator, $\text{S}[\mathcal{M}]$}
\label{verify-correctness-of-simulator-implementation}

When a new probabilistic model $\mathcal{M}$ is introduced \textcolor{blue}{for the first time, a simulator for $\mathcal{M}$ ($\text{S}[\mathcal{M}]$) must be devised and itself validated.
The inferential engine ($\text{I}[\mathcal{M}]$) -- }\textcolor{red}{\st{an its inferential engine ($\text{I}[\mathcal{M}]$) -- }}what users employ in empirical analyses -- \textcolor{blue}{cannot be validated without a valid simulator.}\textcolor{red}{\st{is implemented for the first time, validating $\text{I}[\mathcal{M}]$ requires that a simulator for $\mathcal{M}$, $\text{S}[\mathcal{M}]$, be devised and itself validated.}}
A simulator conventionally requires a parameter value as input (i.e., a value for $\theta$, $\theta$, where $\theta$ might represent the values of more than one parameter), or a prior distribution on those values, $f_{\theta}(\cdot)$. 
Note that we use ``$\cdot$'' when referring specifically to the generative function, rather than the value it takes given input.
The simulator then outputs a sample of random variable(s), which for hierarchical models will include not only an instantiation $d$ of data $\text{D}$, but also the parameters in $\theta$.

\begin{figure}%{l}{5.5cm}
  \centering
  \includegraphics[width=\linewidth]{../figures/graphical_model_manual.pdf}    
  \caption{
    A simple probabilistic graphical (Bayesian) model we will validate in this work.
    (a) When read from top to bottom, the graphical model describes a generative process (see the legend for the meaning of vertical lines and downward-pointing arrows).
    If read from bottom to top, the graphical model describes the process of inference (assuming arrows having opposite orientation denoting the flow of information); in this case, the blue and yellow circles represent the data and the parameters being estimated, respectively.
    A random variable within a rectangular box signifies a parameter whose value is assumed known by the user; these are normally nuisance hyperparameters, or parameters that are not of immediate interest perhaps because they have been estimated elsewhere.
    (b) Each random variable node in the model, and how they should be interpreted.
    Table \ref{tab:dists} presents more detail on each of the sampling distributions.
    Briefly, ``LN'' stands for log-normal, ``Yule'' for a Yule process also known as a pure-birth model, and ``PhyloBM'' stands for a phylogenetic Brownian motion model.
  }  
  \label{fig:pgm} 
\end{figure}
%(\cdot|\Mu_\Lambda=-3.25,\Sigma_\Lambda=0.2)$ 
 
In the case of hierarchical models, it is sometimes useful to consider $\text{S}[\mathcal{M}]$ as a collection of component simulators, each characterized by a different sampling distribution.
For example, \textcolor{blue}{the model $\mathcal{M}$ we will work with below (Fig. \ref{fig:pgm}; Table \ref{tab:dists}) consists of a hierarchical model; parametric distributions are used as hyperpriors (item 1, below), a Yule process is used as the phylogenetic tree prior (item 2), and phylogenetic Brownian motion is used as the data model (item 3):}\textcolor{red}{\st{$\text{S}[\mathcal{M}]$ for model we will work with below can be seen as an ensemble comprised by:}}

\begin{enumerate}
  \item $\text{S}[f_\Theta(\cdot)]$ (where $\Theta = \{T,\Lambda, R, \boldsymbol{Y_0}\}$), which jointly simulates $\theta=\{\tau,\lambda,r,\boldsymbol{y_0}\}$,
  \item $\text{S}[f_{\Phi|T,\Lambda}(\cdot|T=\tau,\Lambda=\lambda)]$, which simulates a Yule tree $\phi$ given an origin age value $\tau$ and a $\lambda$ (the birth-rate) simulated in (1),
  \item $\text{S}[f_{Y|\Phi,R,\boldsymbol{Y_0}}(\cdot|\Phi=\phi,R=r,\boldsymbol{Y_0}=\boldsymbol{y_0})]$, which simulates an array with $n$ continuous-trait values (one value per species), $\boldsymbol{y}$, given a phylogeny $\phi$ with $n$ species, an evolutionary rate $r$ and ancestral character values $\boldsymbol{y_0}$ (simulated in (1) and (2), respectively).
\end{enumerate}

Being able to isolate the building blocks of a hierarchical model simulator helps divide and conquer the validation task, especially when some but not all of the sampling distributions are well-known parametric distributions, or when they result from well characterized stochastic processes (see below).

One way to validate a probabilistic model simulator is by using it to produce (sample) a large number of data sets given a set of parameters.
\textcolor{red}{\st{These parameters can be seen as characterizing an ensemble of the entities being modeled.}}
For each data set, one can then construct $\alpha \times 100 \%$-confidence intervals (where $\alpha \in (0,1)$ gives the confidence level) for certain summary statistics (e.g., mean, variance, covariance).
If the simulator is behaving as expected, one should be able to verify that the (ensemble's or ``true'') summary statistic is contained approximately $\alpha$\% of the time within their $\alpha \times 100\%$-confidence intervals.
An example is the Yule model (also known as the pure-birth model; \citealt{yule24}), a continuous-time Markov process that has been classically employed in phylogenetics to model the number of species in a clade \citep{yule24,aldous01}.
Under a Yule process with a species birth rate of $\lambda$, the expected tree height, $\text{E}[t_{\text{root}}]$, for a tree with $n$ tips is:

\begin{equation}
  \text{E}[t_{\text{root}}] = \sum_{i=2}^{n}\frac{1}{i\lambda}.
  \label{eq:yule}
\end{equation}

\begin{center}
  \begin{table}[h]
  \caption{Sampling distributions used in the probabilistic model validated in this work (Fig. \ref{fig:pgm}).
    Columns ``During simulation'' and ``During inference'' specify how the sampling distributions should be read and interpreted, following the notation in the main text.}
  \label{tab:dists}
  \centering
  \begin{tabular}{ p{1in} p{1in} p{2in} p{2in} }
    \hline
    Label (Fig. \ref{fig:pgm}) & Full name or alias & During simulation & During inference \\
    \hline  
    \rowcolor{gray!10}$\text{LN}_{\Lambda}(-3.25,0.2)$ & Log-normal & $f_{\Lambda | M_\Lambda,\Sigma_\Lambda}(\cdot |M_\Lambda\mathord{=}{-3.25},\Sigma_\Lambda\mathord{=}0.2)$ & $f_{\Lambda | M_\Lambda,\Sigma_\Lambda}(\lambda |M_\Lambda\mathord{=}{-3.25},\Sigma_\Lambda\mathord{=}0.2)$\\
    $\text{LN}_{R}(-3.25,0.2)$ & Log-normal & $f_{R | M_R,\Sigma_R}(\cdot |M_R\mathord{=}{-2.5},\Sigma_R\mathord{=}0.5)$ & $f_{R | M_R,\Sigma_R}(\lambda |M_R\mathord{=}{-2.5},\Sigma_R\mathord{=}0.5)$\\
    \rowcolor{gray!10}Yule & Pure-birth & $f_{\Theta |T,\Lambda}(\cdot |T\mathord{=}\tau,\Lambda\mathord{=}\lambda)$ & $f_{\Theta |\Lambda}(\lambda|T\mathord{=}\tau,\Lambda\mathord{=}\lambda)$\\
    PhyloBM & Phylogenetic Brownian motion & $f_{\boldsymbol{Y} |\Theta,R,Y_0}(\cdot |\Theta\mathord{=}\theta,R\mathord{=}r,Y_0\mathord{=}y_0)$ & $f_{\boldsymbol{Y} |\Theta,R,Y_0}(\boldsymbol{y} |\Theta\mathord{=}\theta,R\mathord{=}r,Y_0\mathord{=}y_0)$
  \end{tabular}
  \end{table}
\end{center}

\noindent One can then verify, for example, if $\text{E}[t_{\text{root}}]$ is $95\%$ of the time within $\pm 1.96$ standard errors of the average Yule-tree root age (from each sampled data set).
Confirming that this is the case indicates $\text{S}[f_{\Phi|\Lambda}(\cdot|T=\tau,\Lambda=\lambda)]$ is correctly implemented (Fig. \ref{fig:yulemean}).
\textcolor{blue}{Interested readers will find another example of this validation procedure in the supplement, where}\textcolor{red}{\st{we illustrate this procedure for}} \textcolor{blue}{the density function characterizing the model (a phylogenetic Brownian motion model, ``PhyloBM'' \citealp{fel73}) is that of a parametric distribution, namely the multivariate normal.}
Protocols for validating $\text{I}[\mathcal{M}]$ (see below) will also normally validate $\text{S}[\mathcal{M}]$ at the same time.

\begin{figure}
  \centering
  \vspace{0pt}
  \begin{subfigure}[t]{0.4\textwidth}
    \caption{}
    \centering
    \begin{tabular}{ cc }
    \hline
    Birth rate ($\lambda$) & E[$t_{\text{root}}$] $\in$ 95\%-CI (\%)\\
    \hline  
    \rowcolor{gray!10}0.5 & 94\\
    0.6 & 91\\
    \rowcolor{gray!10}0.7 & 98\\
    0.8 & 95\\
    \rowcolor{gray!10}0.9 & 96\\
    1.0 & 92
  \end{tabular}
  \end{subfigure}
  \vspace{0pt}
  \hspace{1cm}
  \begin{subfigure}[t]{0.4\textwidth}
    \caption{}
    \centering
    \includestandalone[width=7cm]{../figures/yule_exp_height}
  \end{subfigure}
  \caption{\textcolor{blue}{Validation results for Yule model simulator. (a) How often the expected Yule-tree root age fell within its 95\%-confidence intervals, for different birth rates (see main text for more details). (b) Graphical representation of table in (a).}}
  \label{fig:yulemean}
\end{figure}

We note that we have so far used $\text{S}[\mathcal{M}]$ to represent a \emph{direct} simulator under model $\mathcal{M}$, meaning each and every sample generated by $\text{S}[\mathcal{M}]$ is independent.
\textcolor{blue}{(Researchers wishing to carry out direct simulation will find many examples of capable software in Supplementary Table 1.)}
This is contrast with other simulation strategies, such as conducting MCMC under model $\mathcal{M}$ with no data (i.e., ``sampling from the prior''), given specific parameter values ($\theta$).
This latter approach may be the only option if $\text{S}[\mathcal{M}]$ has not been yet implemented, and it is predicated upon the existence of correct implementations of both an inferential engine $\text{I}[\mathcal{M}]'$ and of proposal functions.
We distinguish $\text{I}[\mathcal{M}]'$ from $\text{I}[\mathcal{M}]$ because simulations are being carried out precisely to validate $\text{I}[\mathcal{M}]$.
Unless MCMC simulations are done with $\text{I}[\mathcal{M}]'$ -- an independent and validated implementation of $\text{I}[\mathcal{M}]$ -- they can introduce circularity to the validation task.

\subsection*{Validating the inferential engine, $\text{I}[\mathcal{M}]$}
\label{sec:sbc}

The more complex the natural phenomenon under study, the more difficult it will be to strike a good balance between model practicality and realism \citep{levins1966}.
The popular aphorism rings true: ``all models are wrong but some are useful'' \citep{box79}.
Very simple models are easier to implement in efficient inference tools, but will commonly make assumptions that are likely to be broken by the data. 
Conversely, complex models will fit the data better, but may become unwieldy with increasing levels of realism.

A large number of parameters can cause overfitting and weak identifiability, and inference under highly complex models might be prohibitively slow \citep{shapiro2000}.
Deciding on the utility of a model for real-world problems is a daunting task \citep{brown18,shepherd18}, and is a challenge we do not address in the present contribution.
Such model appraisals (what we call ``model characterization'' below) are normally carried out after a model is published, often in multiple  contribution bouts, and are critical for a model's longevity.
Analyses of model fit against data are normally accompanied by discussions on assumption validity, and more rarely by benchmarking and scrutinization of model behavior and implementation (e.g., \citealp{maddison07,stadler10,rabosky13,rabosky15,moore16}).

% % Simulating data from a probabilistic generative model can thus be helpful to understand when the inferential machinery under development
When a new model $\mathcal{M}$ is initially proposed, however, authors must ensure that their methods 
can at the very least robustly recover generating parameters.
In this section, we discuss a few techniques that can be employed to assess the correctness of a parameter-estimation routine.
These techniques assume that one can accurately simulate from a
probabilistic data-generating process (see previous section).
\begin{figure}
  \centering
  % % \includestandalone[width=7cm]{../figures/flowchart}
  \includegraphics[width=13cm]{../figures/flowchart_manual2.pdf}
  \caption{Flowchart of the validation of a Bayesian model.
    Standard flowchart symbols are explained in the legend.
    The flowchart area with a clear background is where (true) parameters and data are generated, and where the model simulator(s) is validated.
    The flowchart area shaded in pink mark the steps involved in validating the inference engine once the data has been generated.
    % S($\mathcal{M}$) and I($\mathcal{M}$) denote the simulator and the inference engine of a model $\mathcal{M}$, respectively.
    $\theta$ denotes a vector with $n$ elements, where each element is an i.i.d. parameter(s) sample from its (their) corresponding prior(s) $f_\Theta(\cdot)$.
    Analogously, $\boldsymbol{d}$ denotes a vector with $n$ elements, where each element is an i.i.d. data sample from the corresponding likelihood(s) $f_{D|\Theta}(\cdot|\Theta\mathop{=}\theta)$.
    % Note that we simplify the notation in the main text, with $f_{D|\Theta}(\cdot|\Theta=\theta)$ written simply as $f_{D|\Theta}(\cdot)$.
    $\boldsymbol{\theta}'$ holds $n \times L$ elements, with each being one of the $L$ posterior samples for each of the $n$ parameter samples in $\theta$.
    All $L$ posterior samples obtained from the $i$-th data set $d_i$ comprise together what one would call the posterior distribution over $\theta_i$.
    Posterior samples are commonly obtained through MCMC.
    $\text{U}(l, u)$ denotes a uniform distribution with and including lower and upper bounds $l$ and $u$, respectively.
    The acquamarine dotted box encloses the stages of the pipeline that are exclusive to the coverage validation procedure.
    The pink dotted box encloses the stages of the pipeline that are exclusive to the rank-uniformity validation (RUV) procedure.}
\label{fig:flowchart}
\vspace{2cm}
\end{figure}

\vspace{.25cm}

\noindent{\emph{Coverage validation}}

Our discussion on how to ensure a Bayesian model is well-calibrated and thus correct will mostly follow the ideas in \citet{cook06}
and \citet{Talts2018}.
The basic idea is presented in the flowchart in figure \ref{fig:flowchart} (acquamarine dotted box and what is above it), and consists of three stages: simulation, inference, and coverage calculation.
\textcolor{blue}{Before we delve into more details, the coverage of a parameter is simply how often the (true or simulated) parameter value falls within an estimated Bayesian highest posterior density (HPD) interval.
What constitutes ``acceptable'' coverage, however, depends on the researcher's desired credibility level, $\alpha$.
In what follows, we formally introduce this and other terms and describe a protocol for assessing whether coverage is appropriate.}

\textcolor{blue}{Let us assume}\textcolor{red}{\st{Once}} we have a validated simulator for model $\mathcal{M}$, \textcolor{blue}{and now it is time to validate $\mathcal{M}$'s inferential engine. W}e \textcolor{blue}{will} start by sampling $n$ parameter sets $\boldsymbol{\theta} = \{\theta_i : 1 \leq i \leq n\}$ from its prior,
$f_\Theta(\cdot)$, i.e.:
% \vspace{-1cm}
\begin{align*}
 \theta_i & \sim f_\Theta(\cdot).
\end{align*}
% %   \xout{\theta_0} & \sim \xout{\Pi}

For each parameter set $\theta_i$, we then sample a data set
% \st{$y^{(i)}$}
$d_i$ from
% \st{$f(y^{(i)} \mid \theta^{(i)})$}
$f_{D|\Theta}(\cdot|\Theta=\theta_i)$:

\vspace{-1cm}
\begin{align*}
   d_i & \sim  f_{D|\Theta}(\cdot | \Theta=\theta_i),
\end{align*}
% \xout{\tilde{y}} \sim \xout{F(\cdot \mid \theta_0)}

These two steps conclude the ``simulation'' stage of this validation protocol.
With
% $\boldsymbol{d} = \{d_1,d_2,...,d_n\}$
$\boldsymbol{d} = \{d_i: 1 \leq i \leq n\}$
, we use the inferential machinery $\text{I}[\mathcal{M}]$ under
evaluation to compute
% $p(\theta \mid y^{(i)})$
$f_{\Theta|D}(\theta_i|D=d_i)$ for each $d_i$.
Recall that we assume the posterior distribution defined by $f_{\Theta|D}(\theta|D=d)$ over $\Theta$ will be approximated with MCMC, an algorithm that generates a large sample of size $L$ of parameter values from that posterior distribution, $\boldsymbol{\theta}' = \{ \theta_i^j: 1 \leq i \leq n, 1 \leq j \leq L\}$.
% % $\boldsymbol\theta_s^{(i)} = \{\theta_{s1}^{(i)}, \ldots, \theta_{sL}^{(i)}\}$ of size $L$}.
At this point, we have concluded the inference stage of this validation pipeline.

% \noindent \emph{Well-calibrated validation study}
The third stage and final stage consists of investigating coverage properties of uncertainty intervals.
The critical expectation here is that if the inferential engine is correct, we will be able to obtain interval estimates with precise coverage properties.
More concretely, let us first define the highest posterior density \textcolor{red}{\st{(HPD)}} interval.
For a credibility level $\alpha \in (0, 1)$, we define $I_\alpha(d) := (a(d, \alpha), b(d, \alpha))$
such that:

% %\begin{center}
% %  \hspace{0cm}
% %  \cancel{$\frac{1}{m(y)} \int_{a(y, \alpha)}^{b(y,\alpha)} f(y \mid t)\pi(t)\,dt = \alpha$,}
% %\end{center}

\begin{equation*}
  \frac{1}{f_D(d)} \int_{a(d, \alpha)}^{b(d,\alpha)} f_{D|\Theta}(d | \Theta=\theta)f_\Theta(\theta)\, d\theta = \alpha,
\end{equation*}

\noindent where
% \st{$m(y) = \int_{\Theta} f(y \mid t)\pi(t)\,dt$}
$f_D(d)$
% = \int_\Theta f(d|\theta)f(\theta)d\theta$
is a constant that can be ignored.
By defining $\text{Cred}(I_\alpha(d)) = \alpha$,

\begin{center}
  \hspace{0cm}
  $\inf_{b(d, \alpha)-a(d, \alpha)} \left \{ I_\alpha(d) : \text{Cred}(I_\alpha(d)) = \alpha \right\}$
\end{center}

\noindent yields the shortest interval with the required credibility.
% We are now prepared to discuss how to validate a (Bayesian) inference engine.
% Consider the following generative model:
Note that we approximate a particular $I_\alpha(d_i)$ from the $i$-th $L$ samples obtained with MCMC, in $\boldsymbol{\theta}'$.

Now taking a set of parameter values $\theta_i$ sampled from $f_\Theta(\cdot)$
% respectively,
% \begin{align*}
%  \xout{\theta_0} \theta_1 &\sim \xout{\Pi} f_\Theta(\cdot),\\
%  \xout{\tilde{y}} y_1 & \sim \xout{F(\cdot \mid \theta_0)} f_D(\cdot | \theta_1),
% \end{align*}
it can be shown that
% $\operatorname{Pr}\left(\theta_0 \in I_\alpha(Y) \right) = \alpha$
$\operatorname{Pr}\left(\theta_i \in I_\alpha(d) \right) = \alpha$, i.e., that $100\times\alpha$\% HPDs have nominal coverage under the true generative model (a proof \textcolor{blue}{for any $\alpha$} is provided in the supplementary material).
More formally, the coverage of $n$ intervals obtained as above will be distributed as binomial random variable with $n$ trials and success probability $\alpha$.
When $n=100$ and $\alpha = 0.95$, the 95\%-central interquantile interval for the number of simulations containing the correct data-generating parameter is between $90$ and $99$ (Table \ref{tab:coverage2} \textcolor{blue}{shows the interquantile intervals for other $\alpha$ values.}).
If we ascertain that $\text{I}[\mathcal{M}]$ of a Bayesian model produces coverage lying within the expected bounds, we say the model has passed the coverage validation, and is well-calibrated and correct.

% % \begin{table}
% % \begin{center}
% % \begin{tabular}{l|c}
% % \hline
% % $k$ & $\text{Pr}(x=k)$ \\ % & $\text{P}(x\le k)$ & $\text{P}(x\ge k)$\\
% % \hline
% % 90 & 0.0167 \\ % & 0.0282 & 0.9718\\
% % 91 & 0.0349 \\ % & 0.0631 & 0.9369\\
% % 92 & 0.0649 \\ % & 0.1280 & 0.8720\\
% % 93 & 0.1060 \\ % & 0.2340 & 0.7660\\
% % 94 & 0.1500 \\ % & 0.3840 & 0.6160\\
% % 95 & 0.1800 \\ % & 0.5640 & 0.4360\\
% % 96 & 0.1781 \\ % & 0.7422 & 0.2578\\
% % 97 & 0.1396 \\ % & 0.8817 & 0.1183\\
% % 98 & 0.0812 \\ % & 0.9629 & 0.0371\\
% % 99 & 0.0312 \\ % & 0.9941 & 0.0059\\
% % 100 & 0.0059 \\ % & 1.0000 & 0.0000\\
% % \hline
% % \end{tabular}
% % \end{center}
% % \caption{Under a correctly implemented model, coverage $x$ (the number
% %   of true simulated values that fall within their corresponding 95\%-HPDs)
% %   is binomially distributed with $n$ trials ($n=100$ in this case),
% %   and probability of success $p=0.95$.}
% % \label{tab:coverage}
% % \end{table}

\begin{table}
\begin{center}
\begin{tabular}{cccc}
\hline
Credibility level $\%$ ($100 \times \alpha$) & $n$ (replicates) & Lower quantile & Upper quantile \\ \hline
  \rowcolor{gray!10}   & 100            & 84    & 95    \\
  \rowcolor{gray!10}90 & 200            & 171   & 188   \\
  \rowcolor{gray!10}   & 500            & 436   & 463   \\
                       & 100            & 90    & 99    \\
                  95   & 200            & 184   & 196   \\
                       & 500            & 465   & 484   \\
  \rowcolor{gray!10}   & 100            & 97    & 100   \\
  \rowcolor{gray!10}99 & 200            & 195   & 200   \\
  \rowcolor{gray!10}   & 500            & 490   & 499   \\
                       & 100            & 66    & 83    \\
                 75    & 200            & 138   & 162   \\
                       & 500            & 356   & 394   \\             
  \rowcolor{gray!10}   & 100            & 40    & 60    \\
  \rowcolor{gray!10}50 & 200            & 86    & 114   \\   
  \rowcolor{gray!10}   & 500            & 228   & 272   \\ \hline%\cmidrule(l){1-4} 
\end{tabular}
\end{center}
\caption{The 95\% central interquantile intervals for the number of HPD intervals covering the true parameter value (obtained during coverage validation), under different credibility levels and numbers of replicates.
  Assuming model correctness, the number of true simulated values that fall within their corresponding $100 \times \alpha$\%-HPDs (coverage) is binomially distributed with $n$ trials and probability of success $\alpha$.}
\label{tab:coverage2}
\end{table}


\begin{figure}
  \includegraphics[width=\textwidth]{../figures/graphical_model_coverage.pdf}
  \caption{
    Coverage validation analyses of the Bayesian hierarchical model in Fig. \ref{fig:pgm}.
    Panels show the true (i.e., simulated) parameter values plotted against their mean posteriors (the dashed line shows $x = y$).
    Dots and lines (100 per panel) represent true values and their 95\%-HPDs, respectively.
    Simulations for which 95\%-HPDs contained the true value are highlighted in blue, otherwise are presented in red.
    (a) In ``Scenario 1'', the \textcolor{blue}{model used in inference} was \textcolor{red}{\st{correctly}}\textcolor{blue}{the least miss}specified \textcolor{blue}{(low levels of misspecification were introduced by rejection sampling, when one in ten trees were rejected).}\textcolor{red}{\st{, and we simulated trees with 3 to 300 taxa using rejection sampling (approximately one in ten trees were rejected).} 
    (b) In ``Scenario 2'', the \textcolor{blue}{model used in inference was misspecified beyond the effect of rejection sampling (which was the only source of misspecification in ``Scenario 1'' and ``Scenario 3''); here, }\textcolor{red}{\st{model was incorrectly specified in inference (see main text), and}} we used the same data sets simulated in ``Scenario 1''.
    (c) In ``Scenario 3'' the model was \textcolor{blue}{miss}specified \textcolor{blue}{as a result of rejection sampling}\textcolor{red}{\st{just}} as in ``Scenario 1'', with the difference that \textcolor{blue}{a greater proportion of trees were} reject\textcolor{red}{\st{ion}}\textcolor{blue}{ed}\textcolor{red}{\st{ sampling was substantially greater}} (\textcolor{red}{\st{we rejected a large number of trees, }}approximately 90\% \textcolor{blue}{of trees were rejected, with only those}\textcolor{red}{\st{, keeping those }}having between 100 to 200 tips \textcolor{blue}{being kept}).
  }}
  \label{fig:yulecalval}
\end{figure}

At this point, we will take a moment to remark that the usefulness of model coverage analysis in Bayesian inference is only manifest when $\theta_i$ is sampled from $f_\Theta(\cdot)$.
Method developers may be tempted, for example, to calculate coverage for specific parameter values -- perhaps chosen across a grid over parameter space -- using a different prior during inference.
In such cases, we emphasize that obtaining a coverage lower than 95\% (for 95\% HPDs) does not necessarily mean that a model is incorrectly implemented; conversely, obtaining exactly 95\% coverage does not imply model correctness.
Coverage values \textcolor{red}{\st{only}} have bearing on model correctness if, and only if, random variables are sampled from the same prior distribution used in inference. 

We provide examples of coverage validation attempts in Figure \ref{fig:yulecalval}, which shows coverage graphical summaries for data simulated under the model represented in Figure \ref{fig:pgm}.
This model is deliberately simple for the sake of brevity and clarity in the discussion below.
The parameters in this model are the phylogenetic tree $\Phi$, the species birth rate $\Lambda$, and the continuous-trait evolutionary rate $R$ (we assume the continuous trait value at the root, $\boldsymbol{Y_0}$, is known and set it to $\boldsymbol{0.0}$ for all simulated data sets).
When the model is correctly specified between simulation and inference (``Scenario 1'', Fig. \ref{fig:yulecalval}a), coverage is close to 95\% and adequate for both $\Lambda$ and $R$, which indicates that $\text{I}[\mathcal{M}]$ -- as implemented in BEAST 2, the software we used -- is well-calibrated and correct.

In ``Scenario 2'' of Figure \ref{fig:yulecalval} (Fig. \ref{fig:yulecalval}b), however, we misspecify the model during inference, setting the prior distribution on $\Lambda$ to be a log-normal with a mean of -2.0 (rather than -3.25, as specified in the simulation procedure; Fig. \ref{fig:pgm}).
In contrast with scenario 1, coverage is 0.0 for $\Lambda$ and 70\% for $R$, both much lower and outside the expected coverage bounds (Table \ref{tab:coverage2}).
These numbers indicate that one or more of the parts comprising model $\mathcal{M}$ used in $\text{I}[\mathcal{M}]$ differs from their counterparts in $\text{S}[\mathcal{M}]$.
This result was expected because we purposefully made the models in simulation and inference differ; we know $\text{I}[\mathcal{M}]$ is correct because of the results from scenario 1.
Of course, in a real-world validation experiment the model should be correctly specified, and such a result would suggest a problem with the inferential machinery (provided the the simulator had been previously validated).

Lastly, in ``Scenario 3'' of Figure \ref{fig:yulecalval} (Fig. \ref{fig:yulecalval}c), we specified the model just like in scenario 1, but carried out substantial rejection sampling during simulation.
Approximately 90\% of all simulated trees were rejected based on their taxon count; trees were rejected if they had fewer than 100 or more than 200 taxa.
As with scenario 1, coverage fell within the expected ranges for a correct model implementation.
This result may strike the reader as odd: if $\text{I}[\mathcal{M}]$ expects trees with a wide range of tip numbers, and we feed it simulated trees within a narrow tip number interval, should this not lower coverage?
For example, one may have expected the estimated $\lambda$ to be consistently higher or lower than the true $\lambda$.
$\Lambda$ is nonetheless challenging to infer under the current model, as suggested by estimates falling around the corresponding prior mean value; unlike in scenario 2, however, here the prior mean parameter was correctly specified.
As a result, coverage validation was not capable of detecting any symptoms arising from the rejection of tree samples.

Scenario 3 brings home the point that an incorrect model implementation may pass coverage validation, unless model misspecification is sufficiently severe (e.g., scenario 2), or parameter estimate location is highly responsive to the evidence in the data -- unlike $\Lambda$ in the examined model.
(In the supplement we expand on this point using a different and simpler model, and show that, if extreme, rejection schemes will be detected by coverage validation as a model misspecification issue; Supplementary Table \ref{suptab:bmsimcis}.)
Put simply, obtaining appropriate coverage may not be enough to ascertain that a model is correct.
Potential biases in parameter estimates may remain undetected unless more investigation is done (see rank-uniformity validation below).

The three scenarios we explored above illustrate how coverage validation results can be interpreted in terms of model implementation correctness.
One can additionally capitalize on this validation setup and gauge how accurate our inferential tool can be for different parameters. 
The easier it is to estimate a parameter, the higher should be the correlation between its posterior mean and its generating ``true'' value.
In our scenarios 1 and 3, the species birth rate $\Lambda$ was hard to estimate given the sizes of the phylogenetic trees.
Conversely, the continuous-trait evolutionary rate, $R$, was more easily identifiable, as revealed by the higher correlation between its true values and their posterior means.
We conclude this section by noting that the absence of correlation between parameter estimates and their true values (sometimes referred to as ``weak unidentifiability'') should not be taken as a sign that a model is incorrect -- inappropriate coverage values should.

\vspace{.25cm}

\noindent \emph{Rank-uniformity validation (RUV)}

\cite{Talts2018} showed that one can devise other tests that might be more powerful to detect problems than just looking at the coverage of Bayesian HPD intervals.
In particular, given $\boldsymbol{\theta} = \{\theta_i : 1 \leq i \leq n\}$ (produced according to the protocol in Fig. \ref{fig:flowchart}), those authors demonstrated (Theorem 1 therein) that if the inference machinery $\text{I}[\mathcal{M}]$ works as intended, the distribution of the rank $r_i$ of $\theta_i$ relative to $\boldsymbol{\theta_i'}$ -- i.e., the rank of the $i$-th parameter value relative to its corresponding $L$ MCMC chain samples --
% $\boldsymbol{\theta_i'} = \{\theta_i^j: 1 \leq j \leq L\}$)
will follow a uniform distribution on $[1, L + 1]$ (Fig. \ref{fig:flowchart}, pink dotted box; Fig. \ref{fig:ruv_conceptual}a).
In other words, if one were to sort all true parameter values $\theta_i$ against $\boldsymbol{\theta_i'}$ -- their corresponding $L$ MCMC posterior samples -- the first (smallest ranking) 10\% out of $n$ $\theta_i$ values should account for approximately 10\% of the total rank mass; the next 10\% of (higher ranking) $\theta_i$ values should account, again, for approximately 10\% of the total rank mass, and so on.

% \begin{wrapfigure}{l}{7.35cm}
%   \includegraphics[width=7.25cm]{../figures/flowchart_sbc.pdf}
%   \caption{Flowchart of the rank-uniformity validation procedure,
%     for validating a Bayesian model. The red dotted box encloses the stages of the pipeline that
%   differ from coverage validation (see Fig. \ref{fig:flowchart}).}
% \label{fig:sbcflowchart}
% \end{wrapfigure}

Adherence to this distribution can be investigated by constructing histograms \citep{Talts2018} as well as by looking at the empirical cumulative distribution function (ECDF) and their confidence bands \citep{Sailynoja2021}.
When a model implementation fails RUV, it can do so in different ways.
For instance, when the inference machinery leads to consistent overdispersed estimates, it produces a pattern of ranks concentrating around the middle rank (Fig. \ref{fig:ruv_conceptual}b).
When underdispersion is present, on the other hand, ranks tend to bunch up towards the ends (Fig. \ref{fig:ruv_conceptual}c), creating a pattern of ``horns'', which can also be caused by high autocorrelation in the MCMC draws.
This is also why we recommend thinning MCMC draws in order to reduce autocorrelation.
Figure \ref{fig:ruv_conceptual}d shows the rank patterns when the inference machinery produces biased estimates: ranks will bunch up against one of the ends, depending on whether estimates are biased downwards of upwards.
In the particular case shown in figure \ref{fig:ruv_conceptual}c, the parameter at hand is being overestimated.

\begin{figure}
  \includegraphics[width=\linewidth]{../figures/sbc_conceptual_manual.pdf}
  \caption{
    Patterns observable after inference in rank-uniformity validation (RUV).
    We explain how to interpret the histogram of ranks (middle column) and ECDF plots (right-hand side column) in the main text.
    (a) Model implementation is correct.
    (b) Parameter estimates are overdispersed relative to their true values.
    (c) Parameter estimates are underdispersed relative to their true values.
    (d) Parameter estimates are consistently overestimated relative to their true values.
    In the left-hand side column, the prior and replicate-averaged posterior (also known as the data-averaged posterior) distributions over some parameter $\theta$ are shown in light blue and dark blue, respectively.
    In the middle graphs, light-blue bands represent the 95\%-confidence interval about the expected rank count, and horizontal black lines mark the rank count mean.
    Light-blue ellipses in the rightmost graphs represent confidence intervals about the empirical cumulative distribution function (ECDF).
  }
  % In panel (a) the replicate-averaged posterior (RAP), i.e., the
  % posterior one gets over all \textcolor{blue}{$n \times L$} simulated parameter \textcolor{blue}{values}\textcolor{red}{\st{simulated data sets}}, is overdispersed compared to the prior.
  % This leads to the ranks being concentrated away from the extremes.
  % The scenario in panel (b) shows an underdispersed RAP, which leads to ``horns'' in the histogram plot.
  % Finally, in panel (c) we show a situation where the inference machinery consistently overestimates the true quantity, in which case the ranks are consistently smaller than expected under uniformity.
  \label{fig:ruv_conceptual}
\end{figure}

We conducted RUV on the three scenarios described in the previous section, which make\textcolor{blue}{s} use of the model depicted in Figure 1.
In the interest of brevity, we only show the histograms and ECDFs for $R$, and leave the remaining plots for $\Lambda$ to the supplement (but see Box 1 below).
As expected, under scenario 1 our model implementation passes the RUV -- as indicated by histogram bars and ECDF values falling within their 95\%-confidence intervals (Fig. \ref{fig:ruv_yule}a).

Under scenario 2, again as expected, our method failed RUV (Fig.~\ref{fig:ruv_yule}b).
In particular, we observe\textcolor{red}{\st{s}}\textcolor{blue}{d a} great overestimation of the Brownian motion rate ($R$).
In a real world-analysis, these results would point to one or more faulty implementations (e.g., one or more model components, MCMC machinery, the simulator, etc).
We remind the reader that in our experiment, scenario 2 was purposefully set up so that the (prior) models used in simulation and inference differed; our implementations are actually correct, but were induced to fail the RUV procedure.

Lastly, RUV results for scenario 3 contrasted with what we observed for this scenario's coverage validation (Fig. \ref{fig:yulecalval}c).
While the model specified in scenario 3 passed its coverage validation (coverage was acceptable for both $\Lambda$ and $R$), it did not pass the RUV procedure.
The corresponding rank histogram and ECDF plots indicate that $R$ is underestimated (Fig. \ref{fig:ruv_yule}c).
This result suggests that rank-uniformity validation can be more sensitive than coverage validation, at least for certain types of model misspecification, such as those affecting parameter estimate location.

Unlike scenario 2, in which we caused an explicit mismatch between the distributions used in simulation and inference, model misspecification under scenario 3 was subtler: simulation and inference models were identical (as in scenario 1), but tree samples from $f_\Phi(\cdot)$ (the Yule prior) were often rejected as $\boldsymbol{\theta}$ was generated.
The model used in scenario 3 failed RUV because rejecting tree samples induced an implicit Yule model in simulation that differed from the Yule model used in inference. 
Indeed, using a much simpler model and an analogous rejection scheme (Supplementary Fig. \ref{supfig:ruv_normal_toy}), we were able to recapitulate the results in Figure \ref{fig:ruv_yule}.
While here we focus on so-called continuous parameters like $R$ and $\Lambda$, it is also possible to conduct RUV to assess correctness on the space of phylogenies, a topic we leave for future research.

\begin{figure}
  \centering
  \includegraphics[width=0.95\linewidth]{../figures/sbc_Yule_BM_r_manual.pdf}
  \caption{
    Rank-uniformity validation (RUV) of the Bayesian hierarchical model in Fig. \ref{fig:pgm}.
    Panels in the top row show the histograms of $n=100$ ranks, for parameter $R$ in each scenario, obtained after 10\% burnin and thinning of posterior samples down to 200 out of 10,000.
    Panels in the bottom row show the corresponding ECDF plots, for parameter $R$ in each scenario.
    (a) In ``Scenario 1'', the inference model was \textcolor{blue}{the least miss}\textcolor{red}{\st{ correctly }}specified \textcolor{blue}{(low levels of misspecification were introduced by rejection sampling)} and we can see that the ranks are compatible with a uniform distribution (within the blue band). 
    (b) In ``Scenario 2'', the inference machinery was misspecified \textcolor{blue}{beyond the effect of rejection sampling (which was the only source of misspecification in ``Scenario 1'' and ``Scenario 3''); here, we used the same data sets simulated in ``Scenario 1''. A} clear pattern of overestimation shows up in the ranks, meaning the ranks for the data-generating values are usually smaller than expected under correctness.
    (c) In ``Scenario 3'' we can see a pattern of underestimation, evidenced by ranks bunching up to the right. \textcolor{blue}{Rejection sampling was more extreme in this scenario (i.e., a more misspecified model in inference) than in that shown in (a).}}
  \label{fig:ruv_yule}
\end{figure}

% % Note that a tree by itself does not allow one to simulate the observables (alignments). %TODO decide whether to include this
% % Obviously, one might choose to keep $\boldsymbol \alpha$ fixed during the procedure, but it this will not exactly assess the validity of the joint sampler. 
% % This is particularly important for phylogenetics because the relationship between phylogeny and other parameters non-linear.
% % One important general point about phylogenetic space is that it does not possess a canonical representation.
% % The BHV parametrisation is quite useful in that allows unique geodesics and admits central limit theorems, so maybe there is some theoretical justification for using BHV path lengths as the univariate representation -- i.e. the $\delta$ -- in phylo-SBC.

% % \paragraph{An example}

% % In order to illustrate how a typical phylogenetic SBC analysis might work, we discuss a simple example where [REMCO will describe this in detail].

  
% % There is a wide variation on the validation stringencies different
% % methods and models are subjected to in the course of their
% % development \citep{darriba18}.
% % From a short literature review on Bayesian methods applied to
% % phylogenetics (Table S1), we summarize {\color{red}{four [could be more
% %     after literature review]}} main validation requirements,
% % in increasing order of stringency and comprehensiveness,
% % that model implementations can meet:

% % \begin{enumerate}[i.]
% %   \item the model produces reasonable estimates on a real data set,
% %   \item the model likelihood evaluates to some theoretical
% %     expectation and/or approximates an expected distribution,
% %   \item the model often recovers the right parameter
% %     values -- for one or more parameters (i.e., a small hand-picked
% %     ``grid'' of values) -- from data sets simulated with $S(M)$, and
% % \item the model correctly estimates parameter values from data sets
% %   simulated from prior distributions, as indicated by the estimated
% %   posterior coverage of parameters and their correlation with true
% %   values (``well-calibrated validation'', see below). {\color{red}{If
% %       we want to talk about the next level -- using other HPDs, we can
% %       add it as the 5th possibility and expand on it below}}
% % \end{enumerate}

% % \vspace{.25cm}
% % \noindent \emph{The model produces reasonable estimates on a real
% %   data set}

% % One initial yet insufficient step in validating a model implementation
% % is verifying whether it produces reasonable inferences with a real
% % data set.
% % Depending on the particular biological question at hand, it is very
% % hard or arguably impossible to know what the true, expected answer is,
% % and so ``reasonable'' will always be up for debate.
% % A reasonable inference could be one confirming a result from a previous study
% % using the same data and model, or different data and model
% % altogether, but contradictory inferences do not necessarily imply a model
% % was incorrectly implemented.
% % For this reason, this validation procedure cannot consist of a proper
% % correctness check, but should instead be seen as an exploratory
% % analysis or a sanity check (the latter would require massive
% % evidence supporting a particular hypothesis).
% % Accordingly, a large fraction of publications to date employ this
% % strategy as a final step to illustrate the workings of a method more
% % than to validate it (but see Table S1 in the supplementary material).

% % \vspace{.5cm}
% % \noindent \emph{The model likelihood evaluates to some
% %   expected value or approximates an expected distribution}

% % A good starting point in formally verifying the correctness of a
% % probabilistic model implementation is the comparison between the
% % likelihood of a parameter value given some data, $\text{P}(D|\theta)$, to
% % some expected result.
% % When models are sufficiently simple, or by focusing
% % on a subcase of a complex model, it is often straightforward to derive
% % what this expected result should be (see box 1).
% % Using the Yule model again as an example, the probability of observing
% % a phylogenetic tree $T$ given $n$ internal nodes and birth-rate
% % $\lambda$ (i.e., the Yule's model likelihood function;
% % \citealp{nee01}) is:

% % \begin{equation}
% %   \text{P}(T|n,\lambda) = (n-1)!\lambda^{n-2}e^{-\lambda L},
% %   \label{eq:yulelik}
% % \end{equation}

% % \noindent where $L$ is the total length of the tree.
% % Because likelihood functions are often computed in log-space, and
% % because constants (e.g., the $(n-1)!$ term in Eq. \ref{eq:yulelik}) can be
% % ignored during MCMC for efficiency, the log-likelihood function for the Yule model
% % becomes:

% % \begin{equation}
% % \text{log P}(T|n,\lambda) = \text{log}(\lambda^{n-2}) - \lambda L.
% % \end{equation}

% \noindent One can then easily compute the expected log-likelihood of an example
% tree given some $\lambda$ value by hand, and compare it against the
% value produced by the implementation being validated.
% This forms the basis of what software engineers refer to as ``unit
% testing'' (see the supplementary material for a unit test example
% under the BEAST 2 platform).
% Expected values can also be obtained from independent
% implementations of the likelihood function -- the more different the
% other implementation (e.g., different algorithms and programming
% languages are used), the more robust the test is (see box 1).

% % \vspace{.25cm}
% % \begin{tcolorbox}[breakable, width=\textwidth, colback=gray!10, boxrule=0pt,
% %   title=Box 2: Additional validation sanity-checks, fonttitle=\bfseries]
% %   \small

% %   In addition to the validation procedures described in the main text,
% %   method developers can opt to conduct sanity checks that will not
% %   provide evidence for model correctness, but that can nonetheless
% %   reveal something is wrong with the implementation of a likelihood
% %   function (i.e., checks that are necessary but not sufficient).
  
% %   \vspace{.25cm}
% %   \emph{The score function}
  
% %   One such check is looking at properties of the score function,
% %   $U(\theta,D)=\frac{\partial}{\partial\theta}\log
% %   \text{P}(D|\theta)$.
% %   $U(\theta,D)$ is the gradient of the log-likelihood function with
% %   respect to $\theta$ (the parameters in the model), thus indicating
% %   the slope of $\text{P}(D|\theta)$ and its behavior given very small
% %   changes in $\theta$.
% %   Given a data set $D$ generated from one or more $\theta$ values, it
% %   should be expected from a correct model implementation that:

% %   \begin{equation}\label{eq:scorefunction}
% %     \text{E}[U(\theta,D)] = \int U(\theta,D)\text{P}(D|\theta)dD = 0,
% %   \end{equation}

% %   This theoretical expectation can then be compared to the one
% %   computed from a simulated data set (generated by $S(M)$; see the
% %   supplementary material for more details).

% %   \vspace{.25cm}
% %   \emph{Comparing empirical vs. target posterior distributions}

% %   Given a correctly implemented simulator for model $M$, $S(M)$, one
% %   can directly generate a $\text{P}_{\text{S}}(\theta)$ distribution that
% %   should be approximated by $\text{P}_{\text{I}}(\theta)$ at stationarity.
% %   Note that under this validation scope, there is no data.
% %   If chains are run long enough (as suggested by large effective
% %   sample sizes, ESS's), $\text{P}_{\text{S}}(\theta)$ and
% %   $\text{P}_{\text{I}}(\theta)$ can be compared, and large
% %   discrepancies between them can be taken as evidence of an error in the
% %   likelihood function implementation (assuming all other inferential
% %   components are working properly).

% %   \vspace{.15cm}
% %   Such task can be done with a non-parametric test, for example, such
% %   as the Kolgomorov-Smirnov test \citep{kolgomorov,smirnov,ks}, which
% %   compares two empirical distribution functions (\emph{ECDF}'s) or an
% %   \emph{ECDF} with a cumulative distribution function (\emph{cdf}).
% %   We provide an example of this procedure in the supplementary material.
% % \end{tcolorbox}

% % Alternatively, a more comprehensive approach could involve comparing
% % some expected quantity against its sampling distribution under the
% % model being validated.
% % This sampling distribution is often obtained through MCMC, with
% % newly proposed parameter values being accepted in direct proportion to
% % their likelihood under the model (i.e., this distribution comes from
% % $I(M)$ without using any data, just a fixed $\theta$ value).
% % Given a correct implementation of the Yule model,
% % for example, the distribution of tree heights (for some $\lambda$
% % value) should peak around the value given by from equation \ref{eq:yule}.
% % This procedure is more comprehensive than unit testing because of its
% % dependence on MCMC, which means a valid result requires correct
% % implementations of both the likelihood function and the proposal mechanism.

% % When analytical expectations are not available, the distribution
% % sampled from $I(M)$ with MCMC, $\text{P}_{\text{I}}(\theta)$, can be
% % compared to a target distribution sampled with $S(M)$
% % ($\text{P}_{\text{S}}(\theta)$, see box 2).
% % $S(M)$ can also be itself conveniently used by $I(M)$ as the proposal
% % density during MCMC (see listing 2 in the supplementary material for
% % an example).
% % This allows isolating the validation task to the model:
% % when other proposal densities are used, mismatched distributions can be
% % due to malfunctioning of either or both model and proposal mechanism.

% % \vspace{.5cm}
% % \noindent \emph{The model often recovers the right parameter values
% %   from data sets simulated at a (grid of) point(s) in parameter space}

% % Perhaps the most common approach to date for validating a
% % probabilistic model involves simulating data sets under specific parameter
% % values -- the scope here requires a simulator $S(M)$ used to simulate
% % \emph{data} under model $M$ -- carrying out inference, and tabulating
% % estimates depending on how close they were to the known, true values.
% % This procedure is particularly valuable in the context of point estimates
% % that ignore uncertainty, characteristic of maximum-likelihood methods (e.g.,
% % \citealp{mccormack09,han13,mendes17}), but of smaller if not unclear
% % utility in the Bayesian framework.

% % Differently from frequentist methods, which rely on approximate bootstrap
% % analyses, Bayesian models allow for exact statistical inference while
% % simultaneously addressing uncertainty.
% % As a result, parameter estimates come in the form of a posterior
% % distribution that depends not only on the model likelihood, but
% % also the priors of choice.
% % Validating a Bayesian model by trying to recover
% % hand-picked parameter values can be problematic because it is unclear
% % which prior is to be used, and also how often one should
% % expect a specific highest posterior density (HPD) interval to contain
% % the true parameter values.

% % Presumably, correct model implementations should yield HPDs that contain
% % the true values more often than not, but determining what ``often'' means
% % for complex models and priors is not trivial.
% % Thus, there are no theoretical guarantees that a model is correctly
% % implemented should most inferred 95\% HPDs include the true parameter
% % values, for example, or vice-versa.
% % LM: I thought so too, but it turns out that if you sample from the prior you *can* give guarantees.
% % This validation approach can still be useful in revealing regions
% % in parameter space where inference is easier or harder (given different
% % prior distributions), but we instead recommend procedures that are exact,
% % such as calibrated validation studies (see below).

% % \vspace{.5cm}
% % \noindent \emph{The model correctly estimates parameter values from
% %   data sets simulated from prior distributions, as indicated by the
% %   estimated posterior coverage}

% % Rather than simulating a fixed number of data sets for hand-picked
% % parameter values (see above), we can let the prior density of
% % different values dictate how often they are used in simulations.
% % This forms the basis of a calibrated validation study: a large number
% % of data sets are simulated directly from the prior distributions on
% % model parameters.
% % Here, the choice of prior is arbitrary, but ideally should reflect
% % what is currently known about the natural phenomenon being modeled.

% % The advantage of this approach is that if the same Bayesian model (which
% % includes likelihood \emph{and} priors) is used for both
% % simulation and inference, there are clear expectations under a
% % correctly implemented model.
% % Namely, out of a large number of simulations, the coverage of a
% % parameter should match the width of the HPD of choice
% % (coverage is defined as the number of times the HPD
% % interval contains the true value out of the total number of
% % simulations; \citealp{dawid82}).
% % For example, in the case of the commonly used 95\%-HPD interval, the true value
% % should often be inside the HPD between 90--98\% of the time (Table
% % \ref{tab:coverage2}).


% % Figure \ref{fig:yulecalval} shows the results of a calibrated
% % validation analysis for a simple Bayesian model.
% % Here, the full model is comprised of an exponential prior (with an
% % arbitrary mean of 0.01) on the birth-rate
% % $\lambda$, with the Yule likelihood (i.e., a pure-birth process)
% % modelling speciation times $\tau$ (Fig. \ref{fig:yulecalval}a).
% % We plot 100 ``true'' $\lambda$ values (drawn from the exponential prior)
% % against their posterior means estimated through MCMC from the
% % corresponding trees simulated under the Yule model
% % (Fig. \ref{fig:yulecalval}b-d).
% % Note that coverage is good for the first and second calibrated
% % validation analyses (the vast majority of 95\%-HPDs
% % include the true values, as shown by the blue vertical lines; Fig
% % \ref{fig:yulecalval}b-c), for which the model was correctly
% % specified.
% % Coverage is not good in the third analysis, where the model was
% % mispecified on purpose (a log-normal prior was used in inference,
% % rather than an exponential; Fig. \ref{fig:yulecalval}d).
% % Beyond coverage, one can also determine how good estimates can be,
% % potentially under several simulation scenarios, by investigating the
% % correlation between true parameter values and their posterior means.
% % Correlation should be high when the data carries a lot of
% % signal (Fig. \ref{fig:yulecalval}b), and low otherwise (Fig. \ref{fig:yulecalval}c).

% % Calibrated validation is a multifarious
% % analysis, as it not only verifies the correctness of a model
% % likelihood (both $S(M)$ and $I(M)$), but also of all the components
% % involved in the sampling of the posterior.
% % For this reason, calibrated validation is the most thorough,
% % integrated procedure one can carry out when the goal is to verify
% % implementation correctness.
% % We provide further guidelines and code examples for
% % calibrated validation in the supplementary material.

% % \subsection*{Validating the MCMC transition mechanism (operators)}

% % At the heart of MCMC as an approach to sample a target distribution
% % (conventionally, $\text{P}(\theta|D)$) is
% % the Metropolis-Hastings algorithm \citep{metropolis53,mh}.
% % This algorithm uses a transition mechanism (i.e., a set of
% % ``operators'') characterized by a proposal density $q(\theta'|\theta)$
% % that perturbs parameter values $\theta$ toward new values $\theta'$.
% % The Markov chain progresses as these perturbations are accepted, which
% % occurs with probability $\alpha$:

% % \begin{equation}
% %   \alpha = \text{min}\bigg(1, \frac{\text{P}(\theta'|D)}{\text{P}(\theta|D)} \frac{q(\theta|\theta')}{q(\theta'|\theta)} \bigg),
% % \end{equation}

% % \noindent where $\frac{q(\theta|\theta')}{q(\theta'|\theta)}$ is also
% % referred to as the Hastings ratio \citep{smith93,tierney94,gelman}.
% % For simple proposal densities, the Hastings ratio will often be unity and
% % only the prior and likelihood ratios must be computed.
% % It is sometimes the case, however, that more complex proposals will
% % increase or reduce the dimensionality of parameter space, in which
% % case the derivation of the Hastings ratio will be less straightforward.
% % For the sake of brevity, we point the reader to the relevant theory
% % and examples in \citep{green95,huelsenbeck04,drummond10}.

% % Although operators are not strictly part of the model (as mentioned
% % above, MCMC is not the only way to sample or approximate a target
% % distribution), it is absolutely vital to validate them prior or
% % together with the model per se, should MCMC be the approach of choice.
% % Only correctly implemented operators will lead to ergodic (i.e.,
% % irreducible, positive recurrent, and aperiodic) Markov
% % chains with a stationary distribution that will hopefully match
% % the target distribution, should the chain be long enough.

% % In the context of MCMC, unless a direct simulator $S(M)$ is
% % available to be used as a proposal mechanism (see above), model and operator
% % validation can be seen as two sides of the same coin.
% % Outside of unit-testing, validating models requires carrying out MCMC
% % (see items (ii) and (iv) in the previous section, for example), which
% % in turn can only be a meaningful procedure if correctly implemented
% % operators are available.
% % Conversely, if the intention is to validate new operators, then a
% % model under which to evaluate proposals must have been correctly
% % implemented prior to MCMC.

% % In the latter case, the principle behind validation is the same: one
% % compares the stationary distribution (with respect to one or more summary
% % statistics) obtained using the operator(s)
% % being tested, $\text{P}_{\text{I}}(\theta)$, with an expected
% % distribution, $\text{P}_{\text{S}}(\theta)$ (see listing 5 in
% % supplementary material). 
% % This second, expected distribution can be obtained through MCMC with a
% % mutually exclusive set of correctly implemented operators capable of
% % generating an ergodic chain (see item
% % (a) in supplementary table S1). 

% % Finally, a well-calibrated validation study also serves the purpose of
% % validating the transition mechanism.
% % Low coverage might indicate not only that the model was incorrectly
% % implemented, but also that operators are not functioning as expected.
% % Determining which of the two possibilities -- or whether both are
% % happening -- is not a trivial task, and careful diagnostics are
% % necessary on the part of method developers.

\subsection*{Tree models}

Tree models are stochastic processes that can capture the most fundamental tenet in evolutionary biology, namely common descent, at multiple time scales.
Over the last few decades, pivotal theoretical work has not only characterized many properties of the more elementary tree models (for examples and an overview, see \citealp{nee06,wakeley09,stadler13b,harmon18}), but also generalized them to be more realistic.
Popular among empiricists, for example, are tree models that allow for lineage-affecting event rates that vary over time and across taxa, and that are state-dependent (``state'' here meaning the attributes of a lineage's genotypic, phenotypic, ecological or biogeographic characters).
Such models lend themselves to the study evolutionary phenomena such as species diversification and infectious disease spread.

Although convenient evolutionary abstractions, tree models can nonetheless be challenging to formalize depending on their level of realism.
The parameter space of a tree model is difficult to handle: it includes both a combinatorily complicated discrete component (the tree topology) and a continuous component (the branch lengths) \citep{semple03}.
The theoretical properties, summarization, and exploration of tree space are all active topics of research in mathematical and computational biology \citep{gavryushkina13,gavryushkin16,brown20}.

Given the interest in tree models shown by empirical, computational and theoretical biologists, in this section we will cover how tree models have been and can be validated, with an emphasis on tree space.
We also propose two new ways in which tree models can undergo RUV and be assessed with respect to coverage, respectively.
Our treatment is not meant to be an exhaustive review, but a short synthesis, and in keeping with the subject of the present work, we will not discuss protocols for the development and validation of Bayesian proposals in tree space.
This topical subject is multifaceted (e.g., \citealp{douglas21,bouckaert22,douglas22}) and deserves a dedicated contribution we leave for the future.

One way a tree model implementation can and has been validated is by comparing statistical summaries of its samples (drawn through direct simulation or MCMC without data) against theoretical ``target'' values (e.g., Fig. \ref{fig:yulemean}).
This type of validation can be compelling and is often easy to carry out, but closed-form expressions tend to be only available for simpler models like the birth-death process, the Kingman's coalescent, and a few of their special cases and generalizations.
Such expressions remain useful, nonetheless, as long as more complex models can be constrained to forms for which the relevant theory exists.
Typical theoretical targets include the first moments of distributions on tree characteristics such as internal node ages, internal and terminal branch lengths, the sum of branch lengths, the number of tips, and the frequency of different tree topologies \citep{tj83,rosenberg02,aldous05,nee06,gernhard06,gernhard08,wakeley09,mooers12}.

As tree models increase in complexity (e.g., \citealp{maddison07,goldberg12,fitzjohn10,scire22}), so does the tree space they define and as a consequence theoretical model validation as described above becomes difficult.
When nodes can be serially sampled or direct ancestors of other nodes, for example, even enumerating all the possible trees under a model is a non-trivial exercise \citep{gavryushkina13}. 
The number of t\textcolor{red}{\st{erminal}}\textcolor{blue}{ip}s in a tree can also complicate the theoretical characterization of tree models; except for when the number of t\textcolor{red}{\st{erminal}}\textcolor{blue}{tip}s is small \citep{beast2book}, algorithms have to be employed to generate expectations from theoretical principles \citep{kim19}.

When theoretical predictions useful for validation cannot be made, an often-employed method involves the comparison of independent model implementations, with one or both being simulators or inference engines.
Tree samples from a direct simulator can be compared to samples drawn with MCMC without data (e.g., \citealp{zhang24}), or exact likelihood values can be compared between different implementations \citep{andreoletti22}.
On extreme cases, however, even that strategy appears unattainable.
For example, if it is unclear how to even directly simulate under a tree model -- such as when node-age prior distributions are added to a birth-death process (models used in ``node-dating'', \citealp{ho09}; but see \citealp{heled12}) -- there seems to be no discernible path for validation in tree space.

The procedures of coverage validation and RUV can also be used to examine parameters in tree space.
So far authors have mainly focused on the coverage of quantities such as species tree and gene root ages, sum of branch lengths, and number of direct ancestors (e.g., \citealp{gavryushkina14,ogilvie22,zhang24}).
Fig. \ref{fig:treeheight}, for example, shows the coverage of the root age for the three scenarios we explored in the previous sections.
Similarly to what was observed for tree-unrelated parameters, the root age had the expected coverage in scenarios 1 and 3, and RUV again aligned with the diagnosis of model misspecification for scenario 3.

\begin{figure}
  \centering
  \includegraphics[width=0.95\linewidth]{../figures/treeHeight_manual.pdf}
  \caption{
    Coverage validation and rank-uniformity validation (RUV) of the Bayesian hierarchical model in Fig. \ref{fig:pgm}, for each scenario described in the main text (also see Figs. \ref{fig:yulecalval} and \ref{fig:ruv_yule}), with respect to the height of $\phi$ (i.e., the phylogeny's root age).
    Panels in the top row show the true (i.e., simulated) root age values plotted against their mean posteriors (the dashed line shows $x = y$).
    Dots and lines (100 per panel) represent true values and their 95\%-HPDs, respectively.
    Simulations for which 95\%-HPDs contained the true value are highlighted in blue, otherwise are presented in red.
    Panels in the middle row show the RUV histograms of $n=100$ ranks in each scenario, obtained after 10\% burnin and thinning of posterior samples down to 200 out of 10,000.
    Panels in the bottom row show the corresponding RUV ECDF plots in each scenario.
  }
  \label{fig:treeheight}
\end{figure}

To the best of our knowledge, the RUV procedure has not been applied to validate phylogenetic models \textit{vis-a-vis} tree space.
\textcolor{blue}{
This is likely because, due to the complexity of tree space, it is hard to canonically rank labeled trees such as the ones we focus on here.
Total ordering of unlabeled topologies (shapes) is possible \citep{Colijn2018,Maranca2024} and can be employed when running RUV on this space.
Here, we propose ways of mapping labeled trees to the real line and compute ranks.
For the reasons above, experiments in coverage validation of tree topologies are also exceedingly rare, at least to the degree that we are unaware of any published attempts at it.}
In what follows we address this gap by introducing two novel approaches: (i) a solution for ranking trees as a part of a  RUV analysis , and (ii) a way in which the topology of a tree can have its coverage assessed.

For \textcolor{blue}{the} first method, we propose that each of the trees in a set of MCMC samples, as well as the corresponding ``true'' tree, be first compared to an external, ``reference'' tree (see Algorithm \ref{alg:sbc} in the supplementary material).
For example, a tree sample can be compared to the reference tree with respect to the length of their longest branch, to their topology, to their asymmetry, etc.;
what matters here is that this comparison quantitatively measures the distance between the reference and the other tree.
Then, once one knows how distant each true tree and its posterior samples are from the reference tree, they can be ranked relative to one another based on their associated distance measure.
RUV proceeds normally from this point.

\textcolor{blue}{We illustrate RUV in tree space using a specific functional, or phylogenetic metric, the Robinson-Foulds distance} (RF; \citealp{foulds81})
\textcolor{blue}{ between two trees, which counts the number of clades implied by one tree but not the other.}
\textcolor{blue}{For our purposes, computing the RF distance requires having a reference phylogeny $\phi_0$ to which we can compare our focal generating phylogeny $\phi$ and its posterior MCMC samples.
The RUV protocol remains the same, with an additional step in which we generate $\phi_0$ (see Algorithm} \ref{alg:sbc} \textcolor{blue}{in the supplementary material);
results for five-taxon Kingman coalescent trees (under a known effective population size of 1.0) can be seen in figure} \ref{fig:rf}.
\textcolor{blue}{Figure \ref{fig:rf}a shows the coverage of 95\%-HPD intervals of the RF distance metric, while figure \ref{fig:rf}b-c give its rank distribution and empirical CDF, respectively.
The coverage of the RF metric is very close to 95 (see Table \ref{tab:coverage2}, with $n=100$), and the rank distribution is approximately uniform on (1, $L+1$); together, these panels indicate this model is correctly implemented.}

\textcolor{blue}{Here, there are a few points worth noting. First,}\textcolor{red}{\st{Note that}} \textcolor{blue}{ for large numbers of species, $\phi_0$ is unlikely to share internal nodes with $\phi$'s posterior samples }\citep{steel1988}\textcolor{blue}{, in which case the RF distance metric may not be so useful.
In the supplement we consider other phylogenetic tree metrics that could be used as an alternative or in addition to the RF distance (Supplementary Fig. 4).
Each of these metrics capture different features of parameter space, are computed at varying computational costs (e.g., the BHV$_0$ metric is more costly than the RF distance; Supplementary Table 4) and may be more or less useful in revealing problems with a tree model implementation.
We leave a more detailed comparison of such tree statistics for a future investigation.}

\begin{figure}
  \centering
  \includegraphics[width=0.92\linewidth]{../figures/coverage_ruv_coal_RF.pdf}
  \caption{
    Coverage validation and rank-uniformity validation (RUV) of a Kingman's coalescent model with respect to the Robinson-Foulds distance (RF; see text in box) between the coalescent tree ($\Phi$) and a reference (random) tree ($\phi_0$).
    The effective population size parameter is assumed known and fixed to 1.0 during inference.
    (a) The true RF distances (i.e., between 100 simulated coalescent trees, $\boldsymbol{\phi} = \{\phi_i: 1 \leq i \leq 100\}$, and the same reference tree, $\phi_0$) plotted against the corresponding mean posterior RF distances (calculated from posterior samples of each $\phi_i$ and $\phi_0$).
    The dashed line shows $x = y$.
    Dots and vertical lines represent true RF-distance values and their estimated 95\%-HPDs, respectively.
    Simulations for which 95\%-HPDs contained the true value are highlighted in blue, otherwise are presented in red.
    (b) RUV histograms of 100 ranks obtained after 10\% burnin and thinning of posterior samples down to $1000$ out of $10, 000$.
    (c) RUV empirical CDF plots.}
  \label{fig:rf}
\end{figure}

% (see Box 2 for an example using the Robinson-Foulds distance; section 3 in the supplementary material illustrates RUV and coverage validation with other distance metrics).

% Note that unlike the presentation of the above sections, here we will differentiate between tree model simulator and inferential machinery when necessary as we go.

  % Rank-uniformity validation (RUV) results for the Brownian motion rate, $r$, for each of the scenarios described in the main text.
  % \textcolor{red}{\st{In A we show the rank histograms from 100 replicates}}\textcolor{blue}{(a) Top row: histogram of 100 ranks (given $n=100$ replicates)}, for each scenario, obtained after 10\% burnin and thinning \textcolor{blue}{of posterior samples,} down to\textcolor{red}{\st{ get }} 200 \textcolor{red}{\st{draws from the posterior}} \textcolor{blue}{out of \textcolor{orange}{[Fill out]}}.
  % \textcolor{red}{\st{Panel}} (b) \textcolor{blue}{Bottom row: }\textcolor{red}{\st{shows the corresponding }}ECDF plot, \textcolor{blue}{for each scenario.}\textcolor{red}{\st{which help magnify the problems when they are present (Scenarios 2 and 3).}}
  % \textcolor{red}{\st{For}}\textcolor{blue}{In} Scenario 1, no anomalies are detected\textcolor{red}{\st{, while for}}\textcolor{blue}{. In s}\textcolor{red}{\st{S}}scenarios 2 and 3, over- and underestimation are present, respectively.

% \begin{itemize}
% \item PG1/2: Looking at coverage of tree-space parameters, e.g., root height (continuous space) and number of sampled ancestors (discrete space) (see Fig. 3 in \citealp{gavryushkina14}).
% Also coverage of tree models like the MSC \citep{ogilvie22}.
% % \item PG3: Balance statistics building on Albert Soewongsono's work; TODO: plot ``set'' $\beta$ statistic (y-axis, violin plots) for Yule model with different tree sizes; plot theoretical ``set'' $\beta$ statistic on the x-axis and $\beta$-MLE estimate on y-axis for 100 sets of 100 BiSSE trees (fixed parameters) -- plot 1:1 line and get $\sim$ 95\% CI bars overlapping that line;
% \item PG4: More complicated models are troublesome.
%   Exact likelihood values are computed for sub-cases of a new general model that have theoretical expectations (BDSS) or against an independent implementation \citep{andreoletti22}.
%   Comparing sampling distributions between direct simulator (inverse CDF) and sampling from the prior (BDSS implementation in \citealp{zhang24});
% \end{itemize}

% \begin{tcolorbox}[breakable, width=\textwidth, colback=gray!10, boxrule=0pt, title=Box 2: Validating a phylogenetic model with respect to its phylogenetic tree parameter, fonttitle=\bfseries]
%   \small 

%   Given the centrality of the phylogenetic tree ($\Phi$) in comparative analyses, one must pay close attention to this parameter when validating a phylogenetic model.
%   This is no easy task: tree space is defined by a complex mix of a discrete and continuous components, and one in which trees cannot be easily ranked (see main text).

%   \vspace{.25cm}

%   To get around this difficulty, we propose computing one or more phylogenetic metrics, or functionals, for which total-ordering holds and for which ranks can thus be obtained.
%   One such metric is the well-known Robinson-Foulds distance between two trees (RF; \citealp{foulds81}), which counts the number of clades implied by one tree but not the other.
%   In order to compute a relational metric like the RF distance during validation, we must have a reference phylogeny $\phi_0$ to which we can compare our focal generating phylogeny $\phi$ and its posterior MCMC samples.
%   The RUV protocol remains the same, with an additional step in which we generate $\phi_0$ (see Algorithm \ref{alg:sbc} in the supplementary material).
%   Fig. 9 shows validation results for the RF metric for five-taxon trees simulated under a simple Kingman's coalescent model assuming a known effective population size of 1.0.


%   Fig. 9a shows the coverage of 95\%-HPD intervals of the RF distance metric, while Fig. 9b-c give its rank distribution and empirical CDF, respectively.
%   The coverage of the RF metric is very close to 95 (see Table 3, with $n=100$), and the rank distribution is approximately uniform on (1, $L+1$); together, these panels indicate this model is correctly implemented.
%   Note that for large numbers of species, $\phi_0$ is unlikely to share internal nodes with $\phi$'s posterior samples \citep{steel1988}, in which case the RF distance metric may not be so useful.
%   In the supplement we consider other phylogenetic tree metrics that could be used as an alternative or in addition to the RF distance (Supplementary Fig. 4).
% \end{tcolorbox}

The second validation strategy we introduce allows one to evaluate the coverage of a phylogenetic tree's topology by looking at statistics of its clades.
The procedure verifies that, across all $n$ independently simulated trees, true clades (i.e., clades present in the simulated trees) are sampled in proportion to their fraction among all clades sampled as frequently.
Put differently, one expects that true clades comprise 10\% of all clades with posterior support of 0.1, 20\% of all clades with posterior support of 0.2, and so on (Supplementary Fig. 7).
Importantly, this validation method requires that the tree model only generates trees of the same size, which limits its wide application to all existing tree models.
In the supplement we \textcolor{red}{\st{further}} suggest statistical tests for verifying the adherence of a tree model implementation to the aforementioned expectations\textcolor{red}{\st{.}}\textcolor{blue}{; we also further discuss the interpretation of this validation method's output.}

\subsection*{Software}

We implemented a suite of methods for automating many of the steps involved in coverage validation and RUV.
These methods were developed in Java and integrated into the BEAST 2 platform \citep{beast25}.
A worked example that demonstrates the tools available in BEAST 2 and the Experimenter package can be found in Supplement Section 7.
Code and a tutorial are available on \href{https://github.com/rbouckaert/DeveloperManual}{https://github.com/rbouckaert/DeveloperManual}.

\section*{Bayesian model validation guidelines for developers and reviewers}

In the previous sections, we described and executed two procedures for validating Bayesian models, namely, coverage validation and rank-uniformity validation (RUV).
\textcolor{red}{\st{Once both simulation and inference can be conducted under a model $\mathcal{M}$, executing these procedures amounts to following relatively straightforward protocols (Fig.}} \ref{fig:flowchart}\textcolor{red}{\st{).}}
\textcolor{red}{\st{Importantly, f}}\textcolor{blue}{F}ollowing these procedures should validate any Bayesian model \textcolor{blue}{$\mathcal{M}$ that (i) can be defined in explicit terms as introduced under the ``Probabilistic models'' section, (ii) can generate synthetic data, (iii) can be used in statistical inference while having the same exact mathematical form as the model used in (ii) (i.e., can be correctly specified).
If a model meets the three aforementioned requirements, it can shown to be correct\textcolor{red}{\st{validated}} by the procotols illustrated in figure} \ref{fig:flowchart}, regardless of the nature of its parameter space and its component sampling distributions.

\textcolor{red}{\st{This is b}}\textcolor{blue}{B}ecause \textcolor{red}{\st{such}}\textcolor{blue}{the} protocols \textcolor{blue}{described above} provide clear, objective rules for assessing model correctness\textcolor{red}{\st{, based on the coverage and rank distribution of parameters values; both can be computed for any and all Bayesian models. For the reasons above}}, carrying out an analysis of coverage and/or of the distribution of parameter-value ranks (with respect to their posterior samples) should on one hand be a requirement, and on another should suffice for introducing a new Bayesian model implementation.

\vspace{.25cm}

\textcolor{blue}{\noindent \emph{Practical guidelines}}

\textcolor{blue}{Much like in Bayesian statistical analyses, where researcher (ourselves included) employ ``default'' or ``common'' priors all too often -- as opposed to carefully crafting models to match the uniqueness of their studied system -- there is also a danger of being over-prescriptive when recommending validation protocol guidelines. 
We thus provide below what we deem to be just a rough starting point for those interested in validating or assessing the validation of Bayesian model implementations.\\
\indent In the examples presented above, we used 100 different data sets (i.e., $n=100$; Fig. \ref{fig:flowchart}), each coming from a unique combination of parameter values drawn from their prior distributions.
There is nothing magical about the number 100, however: it is arbitrary and authors (e.g., \citealp{gavryushkina14,gaboriau20,ogilvie22}) have used because it is pleasantly round as a denominator, allowing for immediate mental calculation of coverage values.
In our experience, $n=100$ has proven to be a large enough number of simulations for detecting model implementation issues, but lower or greater $n$'s can be used (Table \ref{tab:coverage2}), with higher error margins and greater running times as the costs associated to each end of $n$, respectively.\\
\indent Two other numbers to consider when carrying out RUV, in particular, is the number of samples to extract from the output of an MCMC procedure ($L$ in Fig. \ref{fig:flowchart}), as well as the number of bins when displaying ranks as histograms (middle panels in Fig. \ref{fig:ruv_conceptual}).
First, we recommend the effective sample size (ESS; a quantity well known in Bayesian statistics) of the MCMC output as a ceiling for $L$ -- the logic here being that fetching `ESS' equidistant samples from the set of posterior samples will minimize sample autocorrelation.
ESS's of at least 200 have become somewhat of a minimal requirement for Bayesian evolutionary analyses, but larger numbers are preferred.\\
\indent Given a correctly implemented model and that a 95\% confidence interval is used, the heights of approximately 5\% of the histogram bars should fall outside the interval.
This makes any integer that is greater than (as well as a product of) 20 a convenient number of histogram bins.
More generally, bin widths should be adjusted to $L$ so that all bins have at least one, but ideally more rank values falling within its bounds.
Empty (or nearly so) bins suggest bins are too narrow or $L$ is too low.\\
\indent The last and most idiosyncratic feature of a validation experiment to be considered is the ideal size of the simulated data set.
Every model is characterized by a data set size beyond which parameters can be estimated with confidence.
If much larger, a data set can make statistical inference too slow, if much smaller, certain attractive features of model validation cannot be leveraged, and implementation bugs may remain undetected.\\
\indent In the case of Bayesian models, too few data points will cause parameter mean posterior estimates to fall along their prior means (see, for example, $\lambda$ in Fig. \ref{fig:yulecalval}), indicating that true parameter values cannot be learned.
While the theory behind and interpretation of coverage validation and RUV remain unchanged -- i.e., (1-$\alpha$) parameter coverage and uniformity of ranks are still to be expected -- one loses the ability to measure inferential accuracy.
More troubling, however, is the possibility that implementation bugs hide behind mean posterior estimates indistinguishable from the parameter's prior mean.
We therefore recommend data sets be as big as necessary for at least one, but ideally more parameters (e.g., $r$ and root age in Figs. \ref{fig:yulecalval} and \ref{fig:treeheight}) to have their values estimated as different from their prior means.}

\vspace{.25cm}

\noindent \emph{My model implementation failed correctness tests, what now?}

Method developers should expect their software to often fail validation, especially at early development stages, causing the loops in Fig. \ref{fig:flowchart} to be visited many times.
The validation procedure is almost always arduous and repetitive, but very effective in revealing issues and in giving modelers peace of mind when releasing their software.
A correctly implemented inference machinery can nonetheless still fail a validation test if there is some unforeseen form of model misspecification (e.g., truncation, see \textcolor{red}{\st{s}}\textcolor{blue}{``S}cenarios 1\textcolor{blue}'' and \textcolor{blue}{``Scenarios 3''} in Figure~\ref{fig:yulecalval}).
In such cases, a potentially delicate stage of method development begins, when decisions must be made between further testing or software release.

If validation success is marginal or contingent upon a substantially constrained parameter space, or if a Bayesian method has good coverage but fails the demanding RUV (as shown here and elsewhere; \citealp{mchugh22}), further simulation experiments might illuminate the nature of the model misspecification and suggest ways to modify the model.
For example, developers may want to tweak an aspect of simulation, and then repeat RUV in search for regularities in parameter over- or underestimation (e.g., section 3 in the supplement).
When releasing a method despite validation failure, researchers should in the very least be expected to report all attempts made to validate an implementation, why they seemed to fail, and what biases were uncovered, if any.
Ideally, guidelines should be provided for interpreting results obtained with a tool known to be biased.

When confronted with utter validation failure, we urge method developers to resist the temptation of downplaying the importance of the validation effort, and instead ask the hard question of whether their models are reasonable in the first place.
\textcolor{blue}{On occasion, researchers may fail to validate a new model with obvious shortcomings (e.g., }\citealp{ree08}\textcolor{blue}{; see also }\citealp{ree09,goldberg11,matzke22}\textcolor{blue}{) -- or perhaps it is not even clear how to validate the model (e.g., phylogenetic models employing node-age priors; \citealp{ho09}) -- yet that new model may still improve on the total absence of statistical methods, and ultimately teach us something novel about the natural world.}
\textcolor{blue}{However, i}\textcolor{red}{\st{I}}f large numbers of simulations must be rejected so as to obtain realistic data\textcolor{red}{\st{ -- }}\textcolor{blue}{(or data whose probabilities can be calculated), }\textcolor{red}{\st{ -- }}this could be a sign that the model needs to be modified. 
Independent implementations that \textcolor{red}{\st{also }}do not pass validation tests provide further evidence that the issue is potentially in the model assumptions themselves.
\textcolor{blue}{Although h}\textcolor{red}{\st{H}}istorically\textcolor{red}{\st{,}} model design has often gone in the direction of incrementally\textcolor{red}{\st{ conditioning the statistical process so as to match empirical observations}}\textcolor{blue}{generalizing existing models (e.g., nested molecular substitution models; }\citealp{fel04}), \textcolor{red}{\st{but }}re-imagining the model entirely \textcolor{red}{\st{might}}\textcolor{blue}{can sometimes} be the best solution.

\vspace{.25cm}

\noindent \emph{Model characterization}

In addition to the model validation we detailed above, there is an infinite number of ways in which a new or published model can have its behavior inspected.
Researchers may want to know, given a model, how sensitive parameter estimates are to data set size, prior choice, model complexity, violation of model assumptions, to name a few.
Studies have examined how these factors affect estimation accuracy and precision (e.g., \citealp{zhang24,luo23}), as well as the mixing and convergence of MCMC chains (e.g., \citealp{nylander04,zhang24}).
We collectively refer to these examinations as ``model characterization'': any analysis of model behavior beyond assessing its correctness.
Model characterization is rarely carried out to satisfy the curiosity of the theoretician (but see, e.g., \citealp{tuffley97,steel20}); it is instead normally motivated by a model's empirical applications.
These investigations are thus critical for the longevity and popularity of a model, as domain experts will only adopt a model widely if they know when to trust the results and how to interpret them.

It is possible to characterize certain aspects of model behavior while simultaneously verifying its correctness, as discussed in the coverage validation section above.
For example, one can observe how accurate parameter estimates are (e.g., if the points in Fig. \ref{fig:yulecalval} fall on the identity line) under both correctly and incorrectly specified models.
However, the requirement of simulating parameter values from a prior distribution $f_{\Theta}(\cdot)$ during the validation of a model can complicate its characterization.
Depending on the characterization experiment's goals and design, researchers may find themselves rejecting a large fraction of simulated data sets -- perhaps because data sets do not resemble those in real life, or because they are too large to analyze.
But as we showed, rejecting draws in simulation may then be picked up by the validation protocol as an incorrectly implemented model.
This problem can only worsen the more dimensions of parameter space are allowed to vary.
In most cases, it may thus make more sense to first verify model correctness by following the procedures we described above, and then characterize model behavior further in a subsequent batch of analyses.

We conclude this section by proposing that scientists contributing or reviewing a new model ask the following question: Is the contribution at hand carrying out an empirical analysis that will specifically profit from scrutinizing model behavior?
If not, then model characterization efforts will likely serve their purpose better elsewhere, and profit from being shouldered by the scientific community at large.

\section*{Concluding remarks}

In order to keep up with the large amounts of data of different kinds accumulating in public databases, researchers in the life sciences must constantly update their computational tool boxes.
New models are implemented in computational methods every day, but if they are not properly validated, downstream conclusions from using those methods may be void of any significance.

In the present study, we described and executed two distinct validation protocols that verify a Bayesian model has been correctly implemented.
Although we looked at examples from evolutionary biology, specifically statistical phylogenetics, these two simulation-based protocols work for any and all Bayesian models.

We further elaborate on the difference between experiments in model validation versus model characterization.
Newly implemented models can only profit from validation experiments, which are strictly concerned with theoretical expectations (e.g., about coverage) a model must meet if correctly implemented.
Model characterization, on the other hand, is about inspecting model behavior as a variety of data set and model attributes interact; here, exact quantitative predictions may not be theoretically guaranteed.
Such experiments are best designed and justified when empirically motivated.

We hope the guidelines described here can enhance both the release rate and standards of statistical software for biology, by assisting its users, developers and referees in quickly finding common ground when evaluating new modeling work.

\subsection*{Supplementary material}

Data available from the Dryad Digital Repository: doi:10.5061/dryad.xksn02vpk (\url{https://datadryad.org/stash/share/Vy7fwzuD7IOo7sj_3cwTiZol-BBw-SvGcvZRHpNUDvc}).

\subsection*{Acknowledgements}

We would like to thank Michael Landis, Albert Soewongsono, Sean McHugh, Sarah Swiston, Tim Vaughan and Tanja Stadler for comments that improved this manuscript.

\subsection*{Funding}
F.K.M. was supported by Marsden grant 16-UOA-277 and by the National Science Foundation (DEB-2040347).
R.B. was supported by Marsden grant 18-UOA-096.
L.M.C was partially supported by the Coordena√ß√£o de Aperfei√ßoamento de
Pessoal de N√≠vel Superior - Brasil (CAPES) - Finance Code 001, and by the School of Applied Mathematics,  Getulio Vargas Foundation.
A.J.D. was supported by Marsden grant 16-UOA-277.

% % ----------------------------------------------------------------------------------------
% %	REFERENCE LIST
% % ----------------------------------------------------------------------------------------

\bibliography{refs}

\end{document}
