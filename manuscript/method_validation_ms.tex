\documentclass[oneside]{article}

\usepackage{blindtext} % dummy text throughout this template 
\usepackage{graphicx} % figures
\usepackage{wrapfig}
\usepackage{color} % for colored text
\usepackage{colortbl} % for colored text
\usepackage{float} % for forcing Figure placement
\usepackage[breakable]{tcolorbox} % text box
\usepackage{enumerate} % bullet point lists
\usepackage{setspace} % line spacing
\usepackage{soul} % strike through
\usepackage[normalem]{ulem} % strike through keeping emphasis standard
\usepackage{cancel} % diagonal strike through
\usepackage[margin=1in]{geometry} % margins
\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
% \linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aestheticsgins
\usepackage[small,labelfont=bf,up,up]{caption} % Custom captions under/above floats in tables or Figures
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{multirow}
\usepackage{amsmath} % Text in equations
\usepackage{titlesec} % Allows customization of titles
\usepackage{titling} % Customizing the title section
\usepackage[hidelinks]{hyperref} % For hyperlinks in the PDF

\usepackage{tikz}
\usepackage{bm}
\usepackage{scalefnt}
\usetikzlibrary{shapes.geometric, arrows, positioning, decorations.markings, shapes.multipart}
\usetikzlibrary{bayesnet}
\usepackage{standalone}

\usepackage{xr} % cross-referencing files
\externaldocument{method_validation_ms_supp}

\usepackage{subcaption} % for subfigures side by side
\captionsetup[subfigure]{singlelinecheck=false} % to put (a) and (b) at the top-left of subfigures

\usepackage[shortlabels]{enumitem} % customized lists (shortlabels
                                % necessary to have i., ii., etc., in enumerate)
\setlist[itemize]{noitemsep} % Make itemize lists more compact

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text
\usepackage{colortbl} % grey stuff in the tables
\definecolor{LightGrey}{gray}{0.9}

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{Mendes et al. $\bullet$ August 2023 $\bullet$ bio{\color{red}R}$\chi$ve} % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{natbib}
\bibliographystyle{apalike}

\usepackage{libertine}

\setlength\columnsep{20pt}



%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

%\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
%\posttitle{\end{center}} % Article title closing formatting

\title{How to validate a Bayesian evolutionary model} % Article title
%LM: strictly speaking, we're 'validating' the inference machinery.
%% LM: Validating a model involves epistemic considerations that are, as far as I understand, outside the scope of this paper.
\author{\textsc{F\'{a}bio K. Mendes$^{1\dagger*}$}, \textsc{Remco Bouckaert$^{2\dagger}$},\\
\textsc{Luiz M. Carvalho$^{3\dagger}$}, \textsc{Alexei J. Drummond$^{4}$} \\
\small $^1$Department of Biology, Washington University in St. Louis\\
\small $^2$School of Computer Science, The University of Auckland\\
\small $^3$Escola de Matem\'{a}tica Aplicada, Funda√ß\~{a}o Getulio Vargas\\
\small $^4$School of Biological Sciences, The University of Auckland\\
\small
\href{mailto:f.mendes@auckland.ac.nz}{$^*$Corresponding author: f.mendes@wustl.edu}\\
{\small $^\dagger$Authors contributed equally to this work}
% \href{mailto:f.mendes@auckland.ac.nz}{another.email@auckland.ac.nz}
%\and % Uncomment if 2 authors are required, duplicate these 4 lines if more
%\textsc{Jane Smith}\thanks{Corresponding author} \\[1ex] % Second author's name
%\normalsize University of Utah \\ % Second author's institution
%\normalsize \href{mailto:jane@smith.com}{jane@smith.com} % Second author's email address
}
\date{\today} % Leave empty to omit a date
\renewcommand{\maketitlehookd}{%
\begin{abstract}
  \noindent Biology has become a highly mathematical discipline in which probabilistic models play a central role.
  As a result, research in the biological sciences is now dependent on computational tools capable of carrying out complex analyses.
  These tools must be validated before they can be used, but what is
  understood as validation varies widely among methodological contributions.
  This may be a consequence of the still embryonic stage of the literature on statistical software validation for computational biology.
  Our manuscript aims to advance this literature.
  Here, we describe and illustrate good practices for assessing the correctness of a model implementation, with an emphasis on Bayesian methods.
  We also introduce a suite of functionalities for automating validation protocols.
  It is our hope that the guidelines presented here help sharpen the focus of discussions on (as well as elevate) expected standards of statistical software for biology.
\end{abstract}
\centering [Probabilistic model, Bayesian model, model validation, coverage]
}

%----------------------------------------------------------------------------------------

\doublespacing

\begin{document}

% Print the title
\maketitle

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}
The last two decades have seen the biological sciences undergo a major revolution.
Critical technological innovations such as the advent of massive parallel sequencing and the accompanying improvements in computational power and storage have flooded biology with unprecedented amounts of data ripe for analysis.
Not only has intraspecific data from multiple individuals allowed progress in fields like medicine and epidemiology
\citep[e.g.,][]{1000g,humanmicrobiome,neafsey15}, population genetics \citep[e.g.,][]{lynch07,lack16,demanuel16} and disease ecology \citep[e.g.,][]{rosenblum13,bates18}, but now a large number of species across the tree of life have had their genomes sequenced, furthering our understanding of species relationships and diversification \citep[e.g.,][]{pease2016,kawahara19,upham19}.
% martin13,brawand14,jarvis14,novikova16
However, as the old adage goes, with great power comes great responsibility: never has the data available to the average biologist been so abundant, but also never has one been so aware of both its complexity and the necessary care needed to analyze it. 
Almost on par with with data accumulation is the rate at which new computational tools are being proposed, as evidenced by journals entirely dedicated to method advances, methodological sections in biological journals, and computational biology degrees being offered by institutions around the world.

One extreme case is the discipline of evolutionary biology, on which we focus our attention.
While it could be said that many decade-old questions and hypotheses in evolutionary biology have aged well and stood up the test of time (e.g., the Red Queen hypothesis, \citealt{vanvalen73,lively87,morran11,gibson15}; the Bateson-Dobzhansky-Muller model, \citealt{dob36,muller40,hopkins12,roda17}), data analysis practices have changed drastically in recent years, to the point they would likely seem exotic and obscure to an evolutionary biologist active forty years ago. 
In particular, evolutionary biology has become highly statistical, with the development and utilization of probabilistic models now being commonplace.

Models are employed in the sciences for many reasons, and fall within a biological abstraction continuum \citep{servedio14}, going from fully verbal, highly abstract models (e.g., \citealt{vanvalen73}), through proof-of-concept models that formalize verbal models (e.g., \citealt{maynard78,reinhold99,mendes18}), to models that interact directly with data through explicit mathematical functions \citep{yule24,fel73,hky,hudson90}.
Within the latter category, probabilistic models have seen a sharp surge in popularity within evolutionary biology, in conjunction with computational tools implementing them.

Despite the increasing pervasiveness of probabilistic models in the biological sciences, tools implementing such models show large variation not only with respect to code quality (from a software engineering perspective), but also to the provided evidence for correctness~\citep{darriba18}.
This is unsurprising given the challenges in funding software research~\citep{siepel19}, and the multidisciplinary nature of method development.
Much of the relevant information regarding good coding and statistical practices is out of reach of the average computational biologist, as it is spread across a variety of specialized sources, often obfuscated by its technical and theoretical presentation.
The bioinformatics community is thus in dire need of synthetic and accessible resources that provide guidance for code improvement and validation.

Here, we summarize best practices in probabilistic model validation for method developers, with an emphasis on Bayesian methods.
We execute two different validation protocols on variations of a simple phylogenetic model, discuss the results, and expand on how to interpret other potential outcomes.
We further introduce a suite of methods for automating these protocols within the BEAST 2 platform \citep{beast25}.
Lastly, we propose method development guidelines for new model contributions, for researchers and reviewers who expect new software to meet not only a desirable standard, but also a reasonable one.

%------------------------------------------------

\section*{Probabilistic models}
\label{sec:prob_models}

Probabilistic models mathematically formalize natural phenomena
having an element of randomness.
This is done through probability distributions describing both the observed
empirical data -- seen as the result of one or more random instantiations of the modeled process -- as well the model parameters, which abstract relevant, but usually unknown aspects of the phenomenon at hand.
In the domain of evolutionary biology specifically, the historical, stochastic, and highly dimensional nature of evolutionary processes makes the utility of probabilistic models self-evident. 

The central component of a probabilistic model, $\text{Pr}(D=d|\Theta=\theta)$, allows us to describe the probability distribution over the data ($D$) given the model parameters ($\Theta$).
This probability mass function (pmf; or its continuous counterpart, the probability density function, pdf, $f_D(d|\Theta=\theta)$) is sometimes referred to as the likelihood function.
Just for this section, we will abuse and simplify notation, and drop variable subscripts, e.g., we will write $f_D(d|\Theta=\theta)$  as $f(d|\theta)$.
As illustrated in the next sections, probabilistic models can be hierarchical, in which case there may be several likelihood functions.
% % (Fig. \ref{fig:graphmodel}).
In a frequentist statistical framework, $f(d|\theta)$ is the sole component of a probabilistic model, and is maximized across parameter space during parameter estimation and model comparison.

In the present study we focus on Bayesian inference, however, where a probabilistic model $\mathcal{M}$ defines a posterior  probability distribution for its parameters, $f(\theta|d) = \frac{f(d|\theta)f(\theta)}{f(d)}$.
Here, our prior inferences or beliefs about the natural world -- represented by the prior distribution $f(\theta)$ -- are confronted with and updated by the data through the likelihood function (or multiple likelihood functions).
$f(d) = \int_\Theta f(d|\theta)f(\theta)d\theta$, the probability of the data, is also known as the marginal likelihood or the model evidence.
Crucially, a Bayesian model includes a prior, $f(\theta)$: when models are compared, for example, $f(\theta)$ needs to be taken into account when computing the model evidence $f(d)$.

Models routinely used in evolutionary biology are often characterized by continuous parameters, and are normally complex enough to preclude analytical solutions for the posterior density $f(\theta|d)$, mainly due to the intractability of the integral appearing in the denominator -- i.e., the marginal likelihood.
In those cases, one can make use of the fact that $f(d)$ is a constant with respect to the parameters that can be ignored (i.e., $f(\theta|d) \propto f(\theta|d)f(\theta)$), and use techniques like Markov chain Monte Carlo (MCMC) to sample the posterior distribution.
This is because MCMC is usually implemented in the form of the Metropolis-Hastings \citep{metropolis53,mh} algorithm, which only requires the posterior to be evaluated up to a constant.

In practice, the Metropolis-Hastings algorithm samples the posterior
distribution (also referred to as the ``target'' distribution) by means of a
transition mechanism. 
If the proposal distribution generated by this mechanism is irreducible, positive recurrent, and aperiodic, and the resulting chain is long enough, then the sampled posterior distribution will closely approximate the target distribution $f(\theta|d)$ \citep{smith93,tierney94,gelman}.

We will spend time considering MCMC in particular, as it is the commonly chosen technique for obtaining samples from $f(\theta|d)$ under an implementation of model $\mathcal{M}$.
A thorough validation effort thus entails verifying the correctness of (i) the model (i.e., $f(d|\theta)f(\theta)$), and (ii) the components involved in the MCMC transition mechanism.
We note that the latter are not part of the model, however, and it is  possible to sample $f(\theta|d)$ with other techniques such as importance sampling, Laplace approximations~\citep{inla}, or even by converting the sampling problem into an optimization one \citep[e.g.,][]{zhang18}.

Finally, we stress that we are interested in practices for verifying model \emph{correctness}.
There are other tests and diagnostics employed to ensure that a particular MCMC analysis is converging as expected.
Ascertaining whether one or more independent Markov chains have converged to a given posterior distribution is not a correctness test, as that distribution might be very different from the target distribution. 
We refer the reader interested in these and related topics to \cite{rwty}, \cite{fabreti2022},  \cite{magee2023} and references therein.

\section*{Validating a Bayesian model}

In this section we discuss a host of procedures for validating an implementation of a Bayesian model.
This entails validating both the (data) simulator and the inferential engine.

\subsection*{Validating the simulator, $\text{S}[\mathcal{M}]$}
\label{verify-correctness-of-simulator-implementation}

When a probabilistic model $\mathcal{M}$ is implemented for the first time, a simulator $\text{S}[\mathcal{M}]$ must be devised and itself validated before we can validate an inferential engine $\text{I}[\mathcal{M}]$.
It is $\text{I}[\mathcal{M}]$ that will be employed by users in empirical analyses.

\begin{figure}%{l}{5.5cm}
  \centering
  \includegraphics[width=12cm]{../figures/graphical_model_manual.pdf}    
  \caption{
    A simple probabilistic graphical (Bayesian) model we will validate in this work.
    When read from top to bottom, the graphical model describes a generative process (see the legend for the meaning of vertical lines and downward-pointing arrows).
    If read from bottom to top, the graphical model describes the process of inference (assuming arrows having opposite orientation denoting the flow of information); in this case, the blue and yellow circles represent the data and the parameters bein estimated, respectively.
  }  
  \label{fig:pgm} 
\end{figure}
%(\cdot|\Mu_\Lambda=-3.25,\Sigma_\Lambda=0.2)$ 
A simulator conventionally requires a parameter value as input (i.e., a value for $\Theta$, $\theta$, where $\theta$ might represent the values of more than one parameter), or a prior distribution on those values, $f_\Theta(\cdot)$. 
Note that we use ``$\cdot$'' when referring specifically to the generative function, rather than the value it takes given input.
The simulator then outputs a sample of random variable(s), which for hierarchical models will include not only an instantiation $d$ of data $\text{D}$, but also of a subset of the parameters in $\Theta$.
 
In the case of hierarchical models, it is sometimes useful to consider $\text{S}[\mathcal{M}]$ as a collection of component simulators, each characterized by a different sampling distribution.
For example, $\text{S}[\mathcal{M}]$ for model we will work with below (Fig. \ref{fig:pgm}; Table \ref{tab:dists}) can be seen as an ensemble comprised by:

\begin{enumerate}
  \item $\text{S}[f_\Theta(\cdot)]$ (where $\Theta = \{\mathcal{O},\Lambda, R, \boldsymbol{Y_0}\}$), which jointly simulates $\theta=\{o,\lambda,r,\boldsymbol{y_0}\}$,
  \item $\text{S}[f_{\Phi|\Lambda}(\cdot|\Lambda=\lambda)]$, which simulates a Yule tree $\phi$ given a value of $o$ (the origin age) and $\lambda$ (the birth-rate) simulated in (1),
  \item $\text{S}[f_{Y|\Phi,R,\boldsymbol{Y_0}}(\cdot|\Phi=\phi,R=r,\boldsymbol{Y_0}=\boldsymbol{y_0})]$, which simulates an array with $n$ continuous-trait values (one value per species), $\boldsymbol{y}$, given a phylogeny $\phi$ with $n$ species, an evolutionary rate $r$ and ancestral character values $\boldsymbol{y_0}$ (simulated in (1) and (2), respectively).
\end{enumerate}

Being able to isolate the building blocks of a hierarchical model simulator helps divide and conquer the validation task, especially when some but not all of the sampling distributions are well-known parametric distributions, or when they result from well characterized stochastic processes (see below).

\begin{center}
  \begin{table}[h]
  \caption{Sampling distributions used in the probabilistic model validated in this work (Fig. \ref{fig:pgm}).
    Columns ``During simulation'' and ``During inference'' specify how the sampling distributions should be read and interpreted, following the notation in the main text.}
  \label{tab:dists}
  \centering
  \begin{tabular}{ p{1in} p{1in} p{2in} p{2in} }
    \hline
    Label (Fig. \ref{fig:pgm}) & Full name or alias & During simulation & During inference \\
    \hline  
    \rowcolor{gray!10}$\text{LN}_{\Lambda}(-3.25,0.2)$ & Log-normal & $f_{\Lambda | M_\Lambda,\Sigma_\Lambda}(\cdot |M_\Lambda\mathord{=}{-3.25},\Sigma_\Lambda\mathord{=}0.2)$ & $f_{\Lambda | M_\Lambda,\Sigma_\Lambda}(\lambda |M_\Lambda\mathord{=}{-3.25},\Sigma_\Lambda\mathord{=}0.2)$\\
    $\text{LN}_{R}(-3.25,0.2)$ & Log-normal & $f_{R | M_R,\Sigma_R}(\cdot |M_R\mathord{=}{-2.5},\Sigma_R\mathord{=}0.5)$ & $f_{R | M_R,\Sigma_R}(\lambda |M_R\mathord{=}{-2.5},\Sigma_R\mathord{=}0.5)$\\
    \rowcolor{gray!10}Yule & Pure-birth & $f_{\Theta |\Lambda}(\cdot |\Lambda\mathord{=}\lambda)$ & $f_{\Theta |\Lambda}(\lambda|\Lambda\mathord{=}\lambda)$\\
    PhyloBM & Phylogenetic Brownian motion & $f_{\boldsymbol{Y} |\Theta,R,Y_0}(\cdot |\Theta\mathord{=}\theta,R\mathord{=}r,Y_0\mathord{=}y_0)$ & $f_{\boldsymbol{Y} |\Theta,R,Y_0}(\boldsymbol{y} |\Theta\mathord{=}\theta,R\mathord{=}r,Y_0\mathord{=}y_0)$\\
    \hline 
  \end{tabular}
  \end{table}
\end{center}

One way to validate a probabilistic model simulator is by using it to produce (sample) a large number of data sets given a set of parameters.
These parameters can be seen as characterizing a ``population'' of the entities being modeled.
For each data set, one can then construct $\alpha \times 100 \%$-confidence intervals (where $\alpha \in (0,1)$ gives the confidence level) for certain summary statistics (e.g., mean, variance, covariance).
If the simulator is behaving as expected, one should be able to verify that the (population or ``true'') summary statistic is contained approximately $\alpha$\% of the time within their $\alpha \times 100\%$-confidence intervals.
An example is the Yule model (also known as the pure-birth model; \citealt{yule24}), a continuous-time Markov process that has been classically employed in phylogenetics to model the number of species in a clade \citep{yule24,aldous01}.
Under a Yule process with a species birth rate of $\lambda$, the expected tree height, $\text{E}[t_{\text{root}}]$, for a tree with $n$ tips is:
\vspace{.5cm}


% Start: BM BOX
\begin{tcolorbox}[breakable, width=\textwidth, colback=gray!10, boxrule=0pt,
  title=Box 1: Models characterized by well-known parametric distributions, fonttitle=\bfseries]
  \small 
  One commonly used model in macroevolution for the study of continuous traits is the phylogenetic Brownian motion model (``PhyloBM'' in Fig. ~\ref{fig:pgm}; \citealt{fel73} ).
The pdf characterizing this model's sampling distribution is in fact the pdf of the multivariate normal (MVN) probability distribution:
\begin{equation}
  \begin{split}
    \text{log }f(\boldsymbol{y} \mid \boldsymbol{y_0}, r, \boldsymbol{T}) = -\frac{1}{2} \Big[ n\text{log}(2\pi) + \text{log}|r \boldsymbol{T}| \Big] & \\
    -\frac{1}{2} \Big[ (\mathbf{y} - \boldsymbol{y_0})^T (r \boldsymbol{T})^{-1} (\mathbf{y} - \boldsymbol{y_0}) \Big],
  \label{eq:bm}
  \end{split}
\end{equation}

\noindent where $\boldsymbol{y}$ corresponds to the observed values of a trait scored for $n$ species, $\boldsymbol{y_0}$ is the trait value at the root of the tree, $r$ is the instantaneous rate of change (i.e., the evolutionary rate, and sometimes represented by $\sigma^2$), and $r\boldsymbol{T}$ is the variance-covariance matrix.
$\boldsymbol{T}$ is a matrix whose elements are deterministically defined by tree $\phi$'s topology and branch lengths; see Fig. 1 below).

\vspace{.25cm}
The probability density function in equation \eqref{eq:bm} describes the distribution that would result from an infinite number of BM ``experiments'' (each experiment being non-mean-reverting, and representing an independent evolutionary trajectory).
Under this model, $\theta = \{\boldsymbol{y_0}, r, \boldsymbol{T}\}$ and $d = \{\boldsymbol{y}\}$ (but note that sometimes researchers treat $\phi$ and consequently $\boldsymbol{T}$ as data).

\begin{center}
\includegraphics[width=11cm]{../figures/bmsim.pdf}
\label{fig:bmsim}
\captionof{figure}{
  A sample of 1000 draws from a MVN distribution, each
  representing the evolutionary trajectory of one continuous trait along
  the species tree on the left. The root trait value, $\boldsymbol{y_0}$, and the
  evolutionary rate of the process, $r$, were set to 0.0 and 0.1, respectively.
  The panel on the right shows histograms of 1000 trait values sampled
  from the MVN for each species, as well as their covariation.}
\end{center}

% \vspace{.25cm}
\emph{Validating a phylogenetic BM simulator}

The MVN is a well-characterized parametric distribution.
When used as the sampling distribution of the phylogenetic BM process, it explicitly defines the expected trait value for each species ($\boldsymbol{y_0}$), as well as their trait value variances and covariances.
The latter comes from the variance-covariance matrix; for the tree shown in Figure 2 and with $r = 0.1$, this matrix
is:
% % For example, given $\boldsymbol{y_0} = [0.0, 0.0, 0.0]$, $r = 0.1$, and $\boldsymbol{T}$
% % derived from the tree shown in Figure \ref{fig:bmsim}
% % Because the MVN is a known parametric distribution, it is trivial to verify the correctness of a
% % phylogenetic BM model simulator, $\text{S}[f(\mathbf{y}|\boldsymbol{y_0},r,\boldsymbol{T})]$.
% % Figure 1 shows the result of 200 simulations under the MVN
% % with $\boldsymbol{y_0} = [0.0, 0.0, 0.0]$, $r = 0.1$, along the tree shown on
% % the left, which determines the variance-covariance matrix:
\begin{equation}
  r\boldsymbol{T} = 0.1
  \begin{bmatrix}
    6 & 5 & 0\\
    5 & 6 & 0\\
    0 & 0 & 6
  \end{bmatrix}
  \label{eq:mat}
\end{equation}

Together, variance-covariance matrix $r\boldsymbol{T}$ and $\boldsymbol{y_0} = [0.0, 0.0, 0.0]$ characterize a population of phylogenetically related species trait values whose means are 0.0, variances are 6.0, and co-variances are 5.0 (between species ``A'' and ``B'') and 0.0 (between species ``C''and either ``A'' or ``B'').

\vspace{.25cm}

Figure 1 shows the distributions of trait values and their variances and covariances for one sample of one thousand independent realizations of phylogenetic BM processes. 
One can see that the sample's average trait value and the variances and covariances approach their expectations. 
More rigorously, one can follow the method described in the main text and verify that those expectations fall within their 95\% confidence intervals 95\% of the time, as calculated from a large number of samples (Supplementary Fig. \ref{supfig:bmsimcis} and Supplementary Table \ref{suptab:bmsimcis}).

\end{tcolorbox}
% % End: BM BOX

\begin{equation}
  \text{E}[t_{\text{root}}] = \sum_{i=2}^{n}\frac{1}{i\lambda}.
  \label{eq:yule}
\end{equation}

\noindent One can then verify if $\text{E}[t_{\text{root}}]$ is $95\%$ of the time within $\pm 1.96$ standard errors of the average Yule-tree height (from each sampled data set).
Confirming that this is the case indicates $\text{S}[f_{\Phi|\Lambda}(\cdot|\lambda)]$ is correctly implemented (Fig. \ref{fig:yulemean}).
In Box 1, we illustrate this procedure for the (parametric) sampling distribution underlying the phylogenetic Brownian motion model (``PhyloBM''; \citealp{fel73}).
Protocols for validating $\text{I}[\mathcal{M}]$ (see below) will also automatically validate $\text{S}[\mathcal{M}]$.

\begin{figure}
  \centering
  \vspace{0pt}
  \begin{subfigure}[t]{0.5\textwidth}
    \caption{}
    \centering
    \begin{tabular}{ cc }
    \hline
    Birth rate ($\lambda$) & E[$t_{\text{root}}$] $\in$ 95\% CI (\%)\\
    \hline  
    0.5 & 94\\
    0.6 & 91\\
    0.7 & 98\\
    0.8 & 95\\
    0.9 & 96\\
    1.0 & 92\\
    \hline
  \end{tabular}
  \end{subfigure}
  \vspace{0pt}
  \hspace{1cm}
  \begin{subfigure}[t]{0.4\textwidth}
    \caption{}
    \centering
    \includestandalone[width=7cm]{../figures/yule_exp_height}    
  \end{subfigure}
  \hfill
  %LM TODO: think about uncertainty intervals for coverage.
  \caption{Validation of Yule tree simulator.
    (a) Number of simulated data sets (out of 100) for which the
    expected tree height ($t_{\text{root}}$) was inside the 95\% CI
    about its sample average -- if the simulator is correct, we expect
    this number to be between 90 and 99 about 95\% of the time.
    Each data set consisted of 50 twenty-taxon simulated Yule trees.
    (b) The area shaded in light blue represents the
    95\% confidence interval about the average tree height, obtained
    from the 5,000 Yule trees simulated in (a). Simulations were
    carried out with the \texttt{TreeSim} R package \citep{stadler11}.}
  \label{fig:yulemean}
\end{figure}

We note that we have so far used $\text{S}[\mathcal{M}]$ to represent a \emph{direct} simulator under model $\mathcal{M}$ (Table~\ref{tab:sim}), meaning each and every sample generated by $\text{S}[\mathcal{M}]$ is independent.
This is contrast with other simulation strategies, such as conducting MCMC under model $\mathcal{M}$ with no data, given specific parameter values ($\theta$).
This latter approach may be the only option if $\text{S}[\mathcal{M}]$ has not been yet implemented, and it is predicated upon the existence of correct implementations of both an inferential engine $\text{I}[\mathcal{M}]'$ and of proposal functions.
We distinguish $\text{I}[\mathcal{M}]'$ from $\text{I}[\mathcal{M}]$ because simulations are being carried out precisely to validate $\text{I}[\mathcal{M}]$.
Unless MCMC simulations are done with $\text{I}[\mathcal{M}]'$ -- an independent implementation of $\text{I}[\mathcal{M}]$ -- they can introduce circularity to the validation task.

\begin{center}
  \begin{table}[h]
  \caption{A non-exhaustive list of direct simulation software used for various models in evolutionary biology.}
  \label{tab:sim}
  \centering
  \begin{tabular}{ p{0.7in} p{1.5in} p{1in} p{1.3in} }
    \hline
    Software package & Model type & Platform & Reference \\
    \hline  
    \rowcolor{gray!10}Seq-Gen & Molecular sequence evolution models & Standalone & \citealp{rambaut97} \\
    ms & Coalescent model & Standalone & \citealp{hudson02}\\
    \rowcolor{gray!10}msprime & Coalescent model & Python & \citealp{kelleher16}\\
    SLiM & Population genetic models & Standalone & \citealp{haller19}\\    
    \rowcolor{gray!10}TreeSim & Birth-death models & R & \citealp{stadler11}\\
    mvMORPH & Continuous trait evolution models & R & \citealp{clavel15}\\
    \rowcolor{gray!10}diversitree & Several birth-death models & R & \citealp{fitzjohn12}\\
    MASTER & Several birth-death models & BEAST 2 & \citealp{vaughan13}\\
    \rowcolor{gray!10}LPhy & Several evolutionary models & Standalone & \citealp{drummond23}\\ 
    \hline
  \end{tabular}
  \end{table}
\end{center}

\subsection*{Validating the inferential engine, $\text{I}[\mathcal{M}]$}
\label{sec:sbc}

The more complex the natural phenomenon under study, the more difficult it will be to strike a good balance between model practicality and realism~\citep{levins1966}.
The popular aphorism rings true: ``all models are wrong but some are useful''~\citep{box79}.
Very simple models are easier to implement in efficient inference tools, but will commonly make assumptions that are likely to be broken by the data (e.g., \citealp{sullivan97,mendes17,mendes19}). 
Conversely, complex models will fit the data better (e.g., \citealp{ogilvie22}), but may become unwieldy with increasing levels of realism.

A large number of parameters can cause overfitting and weak identifiability, and inference for highly complex models might be prohibitively slow (see e.g., \citealp{lartillot11}).
% % In order to achieve correct inferences, it is of utmost importance to guarantee that inferential machinery returns quantifiably correct answers.
% % In practice, however, this is impossible to achieve due to the models under consideration being almost always badly misspecified for the data at hand.
Deciding on the utility of a model for real-world problems is a daunting task \citep{brown18,shepherd18}, and is a challenge we do not address in the present contribution.
Such model appraisals (what we call ``model characterization'' below) are normally carried out after a model is published, often in multiple  contribution bouts, and are critical for a model's longevity.
Analyses of model fit against data are normally accompanied by discussions on assumption validity, and more rarely by benchmarking and scrutinization of model behavior and implementation (e.g., \citealp{maddison07,stadler10,rabosky13,rabosky15,moore16}).

% % Simulating data from a probabilistic generative model can thus be helpful to understand when the inferential machinery under development
When a new model $\mathcal{M}$ is initially proposed, however, authors must ensure that their methods 
can at the very least robustly recover generating parameters.
In this section, we discuss a few techniques that can be employed to assess the correctness of a parameter-estimation routine.
These techniques assume that one can accurately simulate from a
probabilistic data-generating process (see previous section).
\begin{figure}
  \centering
  % % \includestandalone[width=7cm]{../figures/flowchart}
  \includegraphics[width=14cm]{../figures/flowchart_manual.pdf}
  \caption{Flowchart of the validation of a Bayesian model.
    Standard flowchart symbols are explained in the legend.
    ``Parameter samples'' can be obtained in different ways, the most common being through MCMC sampling.
    The acquamarine dotted box encloses the stages of the pipeline that are exclusive to the coverage validation procedure.
    The pink dotted box encloses the stages of the pipeline that are exclusive to the rank-uniformity validation (RUV) procedure.}
\label{fig:flowchart}
\vspace{2cm}
\end{figure}

\vspace{-1cm}

\noindent{\emph{Coverage validation}}

Our discussion on how to ensure a Bayesian model is well-calibrated and thus correct will mostly follow the ideas in \citet{Cook06} and \citet{Talts2018}.
The basic idea is presented in the flowchart in figure \ref{fig:flowchart} (acquamarine dotted box and what is above it), and consists of three stages: simulation, inference, and coverage calculation.
Once we have a validated simulator for model $\mathcal{M}$, we start by sampling $n$ parameter sets $\boldsymbol{\theta} = \{\theta_i : 1 \leq i \leq n\}$ from its prior,
$f_\Theta(\cdot)$, i.e.:
% \vspace{-1cm}
\begin{align*}
 \theta_i & \sim f_\Theta(\cdot).
\end{align*}
% %   \xout{\theta_0} & \sim \xout{\Pi}

For each parameter set $\theta_i$, we then sample a data set
% \st{$y^{(i)}$}
$d_i$ from
% \st{$f(y^{(i)} \mid \theta^{(i)})$}
$f_{D|\Theta}(\cdot|\Theta=\theta_i)$:

\vspace{-1cm}
\begin{align*}
   d_i & \sim  f_{D|\Theta}(\cdot | \Theta=\theta_i),
\end{align*}
% \xout{\tilde{y}} \sim \xout{F(\cdot \mid \theta_0)}

These two steps conclude the ``simulation'' stage of this validation protocol.
With
% $\boldsymbol{d} = \{d_1,d_2,...,d_n\}$
$\boldsymbol{d} = \{d_i: 1 \leq i \leq n\}$
, we use the inferential machinery $\text{I}[\mathcal{M}]$ under
evaluation to compute
% $p(\theta \mid y^{(i)})$
$f_{\Theta|D}(\theta_i|D=d_i)$ for each $d_i$.
Recall that we assume the posterior distribution defined by $f_{\Theta|D}(\theta|D=d)$ over $\Theta$ will be approximated with MCMC, an algorithm that generates a large sample of size $L$ of parameter values from that posterior distribution, $\boldsymbol{\theta}' = \{ \theta_i^j: 1 \leq i \leq n, 1 \leq j \leq L\}$.
% % $\boldsymbol\theta_s^{(i)} = \{\theta_{s1}^{(i)}, \ldots, \theta_{sL}^{(i)}\}$ of size $L$}.
At this point, we have concluded the inference stage of this validation pipeline.

% \noindent \emph{Well-calibrated validation study}
The third stage and final stage consists of investigating coverage properties of uncertainty intervals.
The critical expectation here is that if the inferential engine is correct, we will be able to obtain interval estimates with precise coverage properties.
More concretely, let us first define the highest posterior density (HPD) interval.
For a credibility level $\alpha \in (0, 1)$, we define $I_\alpha(d) := (a(d, \alpha), b(d, \alpha))$
such that:

% %\begin{center}
% %  \hspace{0cm}
% %  \cancel{$\frac{1}{m(y)} \int_{a(y, \alpha)}^{b(y,\alpha)} f(y \mid t)\pi(t)\,dt = \alpha$,}
% %\end{center}

\begin{equation*}
  \frac{1}{f_D(d)} \int_{a(d, \alpha)}^{b(d,\alpha)} f_{D|\Theta}(d | \Theta=\theta)f_\Theta(\theta)\, d\Theta = \alpha,
\end{equation*}

\noindent where
% \st{$m(y) = \int_{\Theta} f(y \mid t)\pi(t)\,dt$}
$f_D(d)$
% = \int_\Theta f(d|\theta)f(\theta)d\theta$
is a constant that can be ignored.
By defining $\text{Cred}(I_\alpha(d)) = \alpha$,

\begin{center}
  \hspace{0cm}
  $\inf_{b(d, \alpha)-a(d, \alpha)} \left \{ I_\alpha(d) : \text{Cred}(I_\alpha(d)) = \alpha \right\}$
\end{center}

\noindent yields the shortest interval with the required credibility.
% We are now prepared to discuss how to validate a (Bayesian) inference engine.
% Consider the following generative model:
Note that we approximate a particular $I_\alpha(d_i)$ from the $i$-th $L$ samples obtained with MCMC, in $\boldsymbol{\theta}'$.

Now taking a set of parameter values $\theta_i$ sampled from $f_\Theta(\cdot)$
% respectively,
% \begin{align*}
%  \xout{\theta_0} \theta_1 &\sim \xout{\Pi} f_\Theta(\cdot),\\
%  \xout{\tilde{y}} y_1 & \sim \xout{F(\cdot \mid \theta_0)} f_D(\cdot | \theta_1),
% \end{align*}
it can be shown that
% $\operatorname{Pr}\left(\theta_0 \in I_\alpha(Y) \right) = \alpha$
$\operatorname{Pr}\left(\theta_i \in I_\alpha(d) \right) = \alpha$, i.e., that $100\times\alpha$\% HPDs have nominal coverage under the true generative model (a proof is provided in the supplementary material).
More formally, the coverage of $n$ intervals obtained as above will be distributed as binomial random variable with $n$ trials and success probability $\alpha$.
When $n=100$ and $\alpha = 0.95$, the 95\% central interquantile interval for the number of simulations containing the correct data-generating parameter is between $90$ and $99$ (Table \ref{tab:coverage2}).
If we ascertain that $\text{I}[\mathcal{M}]$ of a Bayesian model produces coverage lying within the expected bounds, we say the model has passed the coverage validation, and is well-calibrated and correct.

% % \begin{table}
% % \begin{center}
% % \begin{tabular}{l|c}
% % \hline
% % $k$ & $\text{Pr}(x=k)$ \\ % & $\text{P}(x\le k)$ & $\text{P}(x\ge k)$\\
% % \hline
% % 90 & 0.0167 \\ % & 0.0282 & 0.9718\\
% % 91 & 0.0349 \\ % & 0.0631 & 0.9369\\
% % 92 & 0.0649 \\ % & 0.1280 & 0.8720\\
% % 93 & 0.1060 \\ % & 0.2340 & 0.7660\\
% % 94 & 0.1500 \\ % & 0.3840 & 0.6160\\
% % 95 & 0.1800 \\ % & 0.5640 & 0.4360\\
% % 96 & 0.1781 \\ % & 0.7422 & 0.2578\\
% % 97 & 0.1396 \\ % & 0.8817 & 0.1183\\
% % 98 & 0.0812 \\ % & 0.9629 & 0.0371\\
% % 99 & 0.0312 \\ % & 0.9941 & 0.0059\\
% % 100 & 0.0059 \\ % & 1.0000 & 0.0000\\
% % \hline
% % \end{tabular}
% % \end{center}
% % \caption{Under a correctly implemented model, coverage $x$ (the number
% %   of true simulated values that fall within their corresponding 95\%-HPDs)
% %   is binomially distributed with $n$ trials ($n=100$ in this case),
% %   and probability of success $p=0.95$.}
% % \label{tab:coverage}
% % \end{table}

\begin{table}
\begin{center}
\begin{tabular}{cccc}
\hline
Credibility level $\%$ ($100 \times \alpha$) & $n$ (replicates) & Lower quantile & Upper quantile \\ \hline
  \rowcolor{gray!10}   & 100            & 84    & 95    \\
  \rowcolor{gray!10}90 & 200            & 171   & 188   \\
  \rowcolor{gray!10}   & 500            & 436   & 463   \\
                       & 100            & 90    & 99    \\
                  95   & 200            & 184   & 196   \\
                       & 500            & 465   & 484   \\
  \rowcolor{gray!10}   & 100            & 97    & 100   \\
  \rowcolor{gray!10}99 & 200            & 195   & 200   \\
  \rowcolor{gray!10}   & 500            & 490   & 499   \\ \hline%\cmidrule(l){1-4} 
\end{tabular}
\end{center}
\caption{% Under a correctly implemented model, the number of true simulated values that fall within their corresponding $100 \times \alpha$\%-HPDs (coverage) is binomially distributed with $n$ trials and probability of success $\alpha$.
  The 95\% central interquantile intervals for the number of HPD intervals covering the true parameter value (obtained during coverage validation), under different credibility levels and numbers of replicates.
  Assuming model correctness, the number of true simulated values that fall within their corresponding $100 \times \alpha$\%-HPDs (coverage) is binomially distributed with $n$ trials and probability of success $\alpha$.
  % For the experiments described in figure~\ref{fig:yulecalval}, for instance, one would expect between $90$ and $99$ simulated values to fall within their HPDs.
  }
\label{tab:coverage2}
\end{table}


\begin{figure}
  \includegraphics[width=\textwidth]{../figures/graphical_model_coverage.pdf}
  \caption{
    Coverage validation analyses of the Bayesian hierarchical model in Fig. \ref{fig:pgm}.
    Panels show the true (i.e., simulated) parameter values plotted against their mean posteriors (the dashed line shows $x = y$).
    Dots and lines (100 per panel) represent true values and their 95\%-HPDs, respectively.
    Simulations for which 95\%-HPDs contained the true value are highlighted in blue, otherwise are presented in red.
    (a) In ``Scenario 1'', the model was correctly specified, and we simulated trees with 3 to 300 taxa using rejection sampling (approximately one in ten trees were rejected).
    (b) In ``Scenario 2'', the model was incorrectly specified in inference (see main text), and we used the same data sets simulated in ``Scenario 1''.
    (c) In ``Scenario 3'' the model was specified just as in ``Scenario 1'', with the difference that rejection sampling was substantially greater (we rejected a large number of trees, approximately 90\%, keeping those having between 100 to 200 tips).
  }
  \label{fig:yulecalval}
\end{figure}

At this point, we will take a moment to remark that the usefulness of model coverage analysis in Bayesian inference is only manifest when $\theta_i$ is sampled from $f_\Theta(\cdot)$.
Method developers may be tempted, for example, to calculate coverage for specific parameter values -- perhaps chosen across a grid over parameter space -- using a different prior during inference.
In such cases, we emphasize that obtaining a coverage lower than 95\% (for 95\% HPDs) does not necessarily mean that a model is incorrectly implemented; conversely, obtaining exactly 95\% coverage does not imply model correctness.
Coverage values only have bearing on model correctness if, and only if, random variables are sampled from the same prior distribution used in inference. 

We provide examples of coverage validation attempts in Figure \ref{fig:yulecalval}, which shows coverage graphical summaries for data simulated under the model represented in Figure \ref{fig:pgm}.
This model is deliberately simple for the sake of brevity and clarity in the discussion below.
The parameters in this model are the phylogenetic tree $\Phi$, the species birth rate $\Lambda$, and the continuous-trait evolutionary rate $R$ (we assume the continuous trait value at the root, $\boldsymbol{Y_0}$, is known and set it to $\boldsymbol{0.0}$ for all simulated data sets).
When the model is correctly specified between simulation and inference (``Scenario 1'', Fig. \ref{fig:yulecalval}a), coverage is close to 95\% and adequate for both $\Lambda$ and $R$, which indicates that $\text{I}[\mathcal{M}]$ -- as implemented in BEAST 2, the software we used -- is well-calibrated and correct.

In ``Scenario 2'' of Figure \ref{fig:yulecalval} (Fig. \ref{fig:yulecalval}b), however, we misspecify the model during inference, setting the prior distribution on $\Lambda$ to be a log-normal with a mean of -2.0 (rather than -3.25, as specified in the simulation procedure; Fig. \ref{fig:pgm}).
In contrast with scenario 1, coverage is 0.0 for $\Lambda$ and 70\% for $R$, both much lower and outside the expected coverage bounds (Table \ref{tab:coverage2}).
These numbers indicate that one or more of the parts comprising model $\mathcal{M}$ used in $\text{I}[\mathcal{M}]$ differs from their counterparts in $\text{S}[\mathcal{M}]$ differs.
This result was expected because we purposefully made the models in simulation and inference differ; we know $\text{I}[\mathcal{M}]$ is correct because of the results from scenario 1.
Of course, in a real-world validation experiment the model should be correctly specified, and such a result would suggest a problem with the inferential machinery (provided the the simulator had been previously validated).

Lastly, in ``Scenario 3'' of Figure \ref{fig:yulecalval} (Fig. \ref{fig:yulecalval}c), we specified the model just like in scenario 1, but carried out substantial rejection sampling in simulation.
Approximately 90\% of all simulated trees were rejected based on their taxon count; trees were rejected if they had fewer than 100 or more than 200 taxa.
As with scenario 1, coverage fell within the expected ranges for a correct model implementation.
This result may strike the reader as odd: if $\text{I}[\mathcal{M}]$ expects trees with a wide range of tip numbers, and we feed it simulated trees within a narrow tip number interval, should this not lower coverage?
For example, one may have expected the estimated $\lambda$ to be consistently higher or lower than the true $\lambda$.
$\Lambda$ is nonetheless challenging to infer under the current model, as suggested by estimates falling around the corresponding prior mean value; unlike in scenario 2, however, here the prior mean parameter was correctly specified.
As a result, coverage validation was not capable of detecting any symptoms arising from the rejection of tree samples.

Scenario 3 brings home the point that an incorrect model implementation may pass coverage validation, unless model misspecification is sufficiently severe (e.g., scenario 2), or parameter estimate location is highly responsive to the evidence in the data -- unlike $\Lambda$ in the examined model.
(In the supplement we expand on this point using a different and simpler model, and show that, if extreme, rejection schemes will be detected by coverage validation as a model misspecification issue; Supplementary Table \ref{suptab:bmsimcis}.)
Put simply, obtaining appropriate coverage may not be enough to ascertain that a model is correct.
Potential biases in parameter estimates may remain undetected unless more investigation is done (see rank-uniformity validation below).

The three scenarios we explored above illustrate how coverage validation results can be interpreted in terms of model implementation correctness.
One can additionally capitalize on this validation setup and gauge how accurate our inferential tool can be for different parameters. 
The easier it is to estimate a parameter, the higher should be the correlation between its posterior mean and its generating ``true'' value.
In our scenarios 1 and 3, the species birth rate $\Lambda$ was hard to estimate given the sizes of the phylogenetic trees.
Conversely, the continuous-trait evolutionary rate, $R$, was more easily identifiable, as revealed by the higher correlation between its true values and their posterior means.
We conclude this section by noting that no correlation between parameter estimates and their true values (i.e., weak unidentifiability) should not be taken as a sign that a model is incorrect -- inappropriate coverage values should.

\vspace{.25cm}

\noindent \emph{Rank-uniformity validation (RUV)}

\cite{Talts2018} showed that one can devise other tests that might be more powerful to detect problems than just looking at the coverage of Bayesian HPD intervals.
In particular, given $\boldsymbol{\theta} = \{\theta_i : 1 \leq i \leq n\}$ (produced according to the protocol in Fig. \ref{fig:flowchart}), those authors demonstrated (Theorem 1 therein) that if the inference machinery $\text{I}[\mathcal{M}]$ works as intended, the distribution of the rank $r_i$ of $\theta_i$ relative to $\boldsymbol{\theta_i'}$ -- i.e., the rank of the $i$-th parameter value relative to its corresponding $L$ MCMC chain samples --
% $\boldsymbol{\theta_i'} = \{\theta_i^j: 1 \leq j \leq L\}$)
will follow a uniform distribution on $[1, L + 1]$ (Fig. \ref{fig:flowchart}, pink dotted box).
In other words, if one were to sort all true parameter values $\theta_i$ against $\boldsymbol{\theta_i'}$ -- their corresponding $L$ MCMC posterior samples -- the first (smallest ranking) 10\% out of $n$ $\theta_i$ values should account for approximately 10\% of the total rank mass; the next 10\% of (higher ranking) $\theta_i$ values should account, again, for approximately 10\% of the total rank mass, and so on.

% \begin{wrapfigure}{l}{7.35cm}
%   \includegraphics[width=7.25cm]{../figures/flowchart_sbc.pdf}
%   \caption{Flowchart of the rank-uniformity validation procedure,
%     for validating a Bayesian model. The red dotted box encloses the stages of the pipeline that
%   differ from coverage validation (see Fig. \ref{fig:flowchart}).}
% \label{fig:sbcflowchart}
% \end{wrapfigure}

Adherence to this distribution can be investigated by constructing histograms \citep{Talts2018} as well as by looking at the empirical cumulative distribution function (ECDF) and their confidence bands \citep{Sailynoja2021}.
When a model implementation fails RUV, it can do so in different ways.
For instance, when the inference machinery leads to consistent overdispersed estimates, it produces a pattern of ranks concentrating around the middle rank (Fig. \ref{fig:ruv_conceptual}a).
When underdispersion is present, on the other hand, ranks tend to bunch up towards the ends (Fig. \ref{fig:ruv_conceptual}b), creating a pattern of ``horns'', which can also be caused by high autocorrelation in the MCMC draws.
This is also why we recommend thinning MCMC draws in order to reduce autocorrelation.
Figure \ref{fig:ruv_conceptual}c shows the rank patterns when the inference machinery produces biased estimates: ranks will bunch up against one of the ends, depending on whether estimates are biased downwards of upwards.
In the particular case shown in figure \ref{fig:ruv_conceptual}c, the parameter at hand is being overestimated.

\begin{figure}
  \includegraphics[width=\linewidth]{../figures/sbc_conceptual_manual.pdf}
  \caption{
    Miscalibration patterns observable after inference in rank-uniformity validation (RUV).
    We explain how to interpret the histogram of ranks (middle column) and ECDF plots (right-hand side column) in the main text.
    (a) Top row: parameter estimates are overdispersed relative to their true values.
    (b) Middle row: parameter estimates are underdispersed relative to their true values.
    (c) Bottom row: parameter estimates are consistently overestimated relative to their true values.
    In the middle graphs, light-blue bands represent the 95\% confidence interval about the expected rank count, and horizontal black lines mark the rank count mean.
    Light-blue ellipses in the rightmost graphs represent confidence intervals about the empirical cumulative distribution function (ECDF).
  }
  % In panel (a) the replicate-averaged posterior (RAP), i.e., the
  % posterior one gets over all \textcolor{blue}{$n \times L$} simulated parameter \textcolor{blue}{values}\textcolor{red}{\st{simulated data sets}}, is overdispersed compared to the prior.
  % This leads to the ranks being concentrated away from the extremes.
  % The scenario in panel (b) shows an underdispersed RAP, which leads to ``horns'' in the histogram plot.
  % Finally, in panel (c) we show a situation where the inference machinery consistently overestimates the true quantity, in which case the ranks are consistently smaller than expected under uniformity.
  \label{fig:ruv_conceptual}
\end{figure}

We conducted RUV on the three scenarios described in the previous section, which make use of the model depicted in Figure 1.
In the interest of brevity, we only show the histograms and ECDFs for $R$, and leave the remaining plots for $\Lambda$ to the supplement (but see Box 1 below).
As expected, under scenario 1 our model implementation passes the RUV -- as indicated by histogram bars and ECDF values falling within their 95\% confidence intervals (Fig. \ref{fig:ruv_yule}a).

Under scenario 2, again as expected, our method failed RUV (Fig.~\ref{fig:ruv_yule}b).
In particular, we observes great overestimation of the Brownian motion rate ($R$).
In a real world-analysis, these results would point to one or more faulty implementations (e.g., one or more model components, MCMC machinery, the simulator, etc).
We remind the reader that in our experiment, scenario 2 was purposefully set up so that the (prior) models used in simulation and inference differed; our implementations are actually correct, but were induced to fail the RUV procedure.

Lastly, RUV results for scenario 3 contrasted with what we observed for this scenario's coverage validation (Fig. \ref{fig:yulecalval}c).
While the model specified in scenario 3 passed its coverage validation (coverage was acceptable for both $\Lambda$ and $R$), it did not pass the RUV procedure.
The corresponding rank histogram and ECDF plots indicate that $R$ is underestimated (Fig. \ref{fig:ruv_yule}c).
This result suggests that rank-uniformity validation can be more sensitive than coverage validation, at least for certain types of model misspecification, such as those affecting parameter estimate location.

Unlike scenario 2, in which we caused an explicit mismatch between the distributions used in simulation and inference, model misspecification under scenario 3 was subtler: simulation and inference models were identical (as in scenario 1), but tree samples from $f_\Phi(\cdot)$ (the Yule prior) were often rejected as $\boldsymbol{\theta}$ was generated.
The model used in scenario 3 failed RUV because rejecting tree samples induced an implicit Yule model in simulation that differed from the Yule model used in inference. 
Indeed, using a much simpler model and an analogous rejection scheme (Supplementary Fig. \ref{supfig:ruv_normal_toy}), we were able to recapitulate the results in Figure \ref{fig:ruv_yule}.
While here we focus on so-called continuous parameters like $R$ and $\Lambda$, it is also possible to conduct RUV to assess correctness on the space of phylogenies, a topic we leave for future research.

\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{../figures/sbc_Yule_BM_r_manual.pdf}
  \caption{
    Rank-uniformity validation (RUV) of the Bayesian hierarchical model in Fig. \ref{fig:pgm}.
    Panels in the top row show the histograms of $n=100$ ranks, for parameter $R$ in each scenario, obtained after 10\% burnin and thinning of posterior samples down to 200 out of 10,000.
    Panels in the bottom row show the corresponding ECDF plots, for parameter $R$ in each scenario.
    (a) In ``Scenario 1'', the model was correctly specified, and we simulated trees with 3 to 300 taxa using rejection sampling (approximately one in ten trees were rejected).
    (b) In ``Scenario 2'', the model was incorrectly specified in inference (see main text), and we used the same data sets simulated in ``Scenario 1''.
    (c) In ``Scenario 3'' the model was specified just as in ``Scenario 1'', with the difference that rejection sampling was substantially greater (we rejected a large number of trees, approximately 90\%, keeping those having between 100 to 200 tips).
  }
  % Rank-uniformity validation (RUV) results for the Brownian motion rate, $r$, for each of the scenarios described in the main text.
  % \textcolor{red}{\st{In A we show the rank histograms from 100 replicates}}\textcolor{blue}{(a) Top row: histogram of 100 ranks (given $n=100$ replicates)}, for each scenario, obtained after 10\% burnin and thinning \textcolor{blue}{of posterior samples,} down to\textcolor{red}{\st{ get }} 200 \textcolor{red}{\st{draws from the posterior}} \textcolor{blue}{out of \textcolor{orange}{[Fill out]}}.
  % \textcolor{red}{\st{Panel}} (b) \textcolor{blue}{Bottom row: }\textcolor{red}{\st{shows the corresponding }}ECDF plot, \textcolor{blue}{for each scenario.}\textcolor{red}{\st{which help magnify the problems when they are present (Scenarios 2 and 3).}}
  % \textcolor{red}{\st{For}}\textcolor{blue}{In} Scenario 1, no anomalies are detected\textcolor{red}{\st{, while for}}\textcolor{blue}{. In s}\textcolor{red}{\st{S}}scenarios 2 and 3, over- and underestimation are present, respectively.
  \label{fig:ruv_yule}
\end{figure}


% % \begin{tcolorbox}[breakable, width=\textwidth, colback=gray!10, boxrule=0pt,
% %   title=Box 1: Validating a phylogenetic model with respect to its
% %   phylogenetic tree parameter, fonttitle=\bfseries]
% %   \small 

% %   Given the centrality of the phylogenetic tree ($\Phi$) in comparative
% %   analyses, we must pay close attention to how we investigate this
% %   parameter when validating a phylogenetic model.
% %   Analyzing phylogenetic trees is made challenging by tree space
% %   being a complex mix of a discrete and continuous component, due to
% %   trees being comprised by both a topology and set of node times
% %   \citep{semple03,gavryushkin16}.
% %   Additionally, there is no canonical total-ordering  structure for
% %   trees, which complicates a validation procedure such as SBC.

% %   \vspace{.25cm}
% %   To get around this difficulty, we propose computing one or more phylogenetic
% %   metrics, or functionals, for which total-ordering holds
% %   and thus for which ranks can be obtained.
% %   \textcolor{red}{[@Luiz: my suggestion is to focus on RF here]}
% %   One such metric is the well-known Robinson-Foulds distance (RF; \citealp{Robinson1981}),
% %   which measures \textcolor{red}{[@Luiz: brief description of RF]}
% %   relative to another phylogenetic tree.
% %   In order to compute a relational metric like the RF distance during
% %   validation, we must have a reference phylogeny $\Phi_0$ to which
% %   we can compare our focal generating phylogeny $\Phi$ and its
% %   posterior MCMC samples.
% %   The simulation-based calibration protocol remains the same, with
% %   an additional step in which we generate $\Phi_0$ (see Algorithm
% %   \ref{alg:sbc} in the supplementary material).
% %   Figure \textcolor{red}{X [@Luiz: see my description of new Fig. X below]} shows
% %   validation results for the RF metric for trees simulated under the model
% %   illustrated in Fig \ref{fig:pgm}. 
% %   Figure \textcolor{red}{Xa} and \textcolor{red}{Xb} show the coverage of 95\%-HPD
% %   intervals, and the rank distribution of the RF metric, respectively.
% %   The coverage of the RF statistic is very close to 95, and the rank distribution
% %   is approximately uniform on (1,$L+1$); together, both panels indicate this model
% %   is correct.
% %   We consider other phylogenetic tree metrics that could be used as an alternative
% %   to RF in the supplementary material (Supplementary
% %   Figs. \ref{supfig:sbc} and \ref{supfig:phylo_calibration}).

% %   \textcolor{red}{[@Luiz: include Fig Xa and FigXb here; this Figure
% %     would consist of just the RF panels; all other metrics have would
% %     be placed in the supp. material]}
% % \end{tcolorbox}

% % Note that a tree by itself does not allow one to simulate the observables (alignments). %TODO decide whether to include this
% % Obviously, one might choose to keep $\boldsymbol \alpha$ fixed during the procedure, but it this will not exactly assess the validity of the joint sampler. 
% % This is particularly important for phylogenetics because the relationship between phylogeny and other parameters non-linear.
% % One important general point about phylogenetic space is that it does not possess a canonical representation.
% % The BHV parametrisation is quite useful in that allows unique geodesics and admits central limit theorems, so maybe there is some theoretical justification for using BHV path lengths as the univariate representation -- i.e. the $\delta$ -- in phylo-SBC.

% % \paragraph{An example}

% % In order to illustrate how a typical phylogenetic SBC analysis might work, we discuss a simple example where [REMCO will describe this in detail].

  
% % There is a wide variation on the validation stringencies different
% % methods and models are subjected to in the course of their
% % development \citep{darriba18}.
% % From a short literature review on Bayesian methods applied to
% % phylogenetics (Table S1), we summarize {\color{red}{four [could be more
% %     after literature review]}} main validation requirements,
% % in increasing order of stringency and comprehensiveness,
% % that model implementations can meet:

% % \begin{enumerate}[i.]
% %   \item the model produces reasonable estimates on a real data set,
% %   \item the model likelihood evaluates to some theoretical
% %     expectation and/or approximates an expected distribution,
% %   \item the model often recovers the right parameter
% %     values -- for one or more parameters (i.e., a small hand-picked
% %     ``grid'' of values) -- from data sets simulated with $S(M)$, and
% % \item the model correctly estimates parameter values from data sets
% %   simulated from prior distributions, as indicated by the estimated
% %   posterior coverage of parameters and their correlation with true
% %   values (``well-calibrated validation'', see below). {\color{red}{If
% %       we want to talk about the next level -- using other HPDs, we can
% %       add it as the 5th possibility and expand on it below}}
% % \end{enumerate}

% % \vspace{.25cm}
% % \noindent \emph{The model produces reasonable estimates on a real
% %   data set}

% % One initial yet insufficient step in validating a model implementation
% % is verifying whether it produces reasonable inferences with a real
% % data set.
% % Depending on the particular biological question at hand, it is very
% % hard or arguably impossible to know what the true, expected answer is,
% % and so ``reasonable'' will always be up for debate.
% % A reasonable inference could be one confirming a result from a previous study
% % using the same data and model, or different data and model
% % altogether, but contradictory inferences do not necessarily imply a model
% % was incorrectly implemented.
% % For this reason, this validation procedure cannot consist of a proper
% % correctness check, but should instead be seen as an exploratory
% % analysis or a sanity check (the latter would require massive
% % evidence supporting a particular hypothesis).
% % Accordingly, a large fraction of publications to date employ this
% % strategy as a final step to illustrate the workings of a method more
% % than to validate it (but see Table S1 in the supplementary material).

% % \vspace{.5cm}
% % \noindent \emph{The model likelihood evaluates to some
% %   expected value or approximates an expected distribution}

% % A good starting point in formally verifying the correctness of a
% % probabilistic model implementation is the comparison between the
% % likelihood of a parameter value given some data, $\text{P}(D|\theta)$, to
% % some expected result.
% % When models are sufficiently simple, or by focusing
% % on a subcase of a complex model, it is often straightforward to derive
% % what this expected result should be (see box 1).
% % Using the Yule model again as an example, the probability of observing
% % a phylogenetic tree $T$ given $n$ internal nodes and birth-rate
% % $\lambda$ (i.e., the Yule's model likelihood function;
% % \citealp{nee01}) is:

% % \begin{equation}
% %   \text{P}(T|n,\lambda) = (n-1)!\lambda^{n-2}e^{-\lambda L},
% %   \label{eq:yulelik}
% % \end{equation}

% % \noindent where $L$ is the total length of the tree.
% % Because likelihood functions are often computed in log-space, and
% % because constants (e.g., the $(n-1)!$ term in Eq. \ref{eq:yulelik}) can be
% % ignored during MCMC for efficiency, the log-likelihood function for the Yule model
% % becomes:

% % \begin{equation}
% % \text{log P}(T|n,\lambda) = \text{log}(\lambda^{n-2}) - \lambda L.
% % \end{equation}

% \noindent One can then easily compute the expected log-likelihood of an example
% tree given some $\lambda$ value by hand, and compare it against the
% value produced by the implementation being validated.
% This forms the basis of what software engineers refer to as ``unit
% testing'' (see the supplementary material for a unit test example
% under the BEAST 2 platform).
% Expected values can also be obtained from independent
% implementations of the likelihood function -- the more different the
% other implementation (e.g., different algorithms and programming
% languages are used), the more robust the test is (see box 1).

% % \vspace{.25cm}
% % \begin{tcolorbox}[breakable, width=\textwidth, colback=gray!10, boxrule=0pt,
% %   title=Box 2: Additional validation sanity-checks, fonttitle=\bfseries]
% %   \small

% %   In addition to the validation procedures described in the main text,
% %   method developers can opt to conduct sanity checks that will not
% %   provide evidence for model correctness, but that can nonetheless
% %   reveal something is wrong with the implementation of a likelihood
% %   function (i.e., checks that are necessary but not sufficient).
  
% %   \vspace{.25cm}
% %   \emph{The score function}
  
% %   One such check is looking at properties of the score function,
% %   $U(\theta,D)=\frac{\partial}{\partial\theta}\log
% %   \text{P}(D|\theta)$.
% %   $U(\theta,D)$ is the gradient of the log-likelihood function with
% %   respect to $\theta$ (the parameters in the model), thus indicating
% %   the slope of $\text{P}(D|\theta)$ and its behavior given very small
% %   changes in $\theta$.
% %   Given a data set $D$ generated from one or more $\theta$ values, it
% %   should be expected from a correct model implementation that:

% %   \begin{equation}\label{eq:scorefunction}
% %     \text{E}[U(\theta,D)] = \int U(\theta,D)\text{P}(D|\theta)dD = 0,
% %   \end{equation}

% %   This theoretical expectation can then be compared to the one
% %   computed from a simulated data set (generated by $S(M)$; see the
% %   supplementary material for more details).

% %   \vspace{.25cm}
% %   \emph{Comparing empirical vs. target posterior distributions}

% %   Given a correctly implemented simulator for model $M$, $S(M)$, one
% %   can directly generate a $\text{P}_{\text{S}}(\theta)$ distribution that
% %   should be approximated by $\text{P}_{\text{I}}(\theta)$ at stationarity.
% %   Note that under this validation scope, there is no data.
% %   If chains are run long enough (as suggested by large effective
% %   sample sizes, ESS's), $\text{P}_{\text{S}}(\theta)$ and
% %   $\text{P}_{\text{I}}(\theta)$ can be compared, and large
% %   discrepancies between them can be taken as evidence of an error in the
% %   likelihood function implementation (assuming all other inferential
% %   components are working properly).

% %   \vspace{.15cm}
% %   Such task can be done with a non-parametric test, for example, such
% %   as the Kolgomorov-Smirnov test \citep{kolgomorov,smirnov,ks}, which
% %   compares two empirical distribution functions (\emph{ECDF}'s) or an
% %   \emph{ECDF} with a cumulative distribution function (\emph{cdf}).
% %   We provide an example of this procedure in the supplementary material.
% % \end{tcolorbox}

% % Alternatively, a more comprehensive approach could involve comparing
% % some expected quantity against its sampling distribution under the
% % model being validated.
% % This sampling distribution is often obtained through MCMC, with
% % newly proposed parameter values being accepted in direct proportion to
% % their likelihood under the model (i.e., this distribution comes from
% % $I(M)$ without using any data, just a fixed $\theta$ value).
% % Given a correct implementation of the Yule model,
% % for example, the distribution of tree heights (for some $\lambda$
% % value) should peak around the value given by from equation \ref{eq:yule}.
% % This procedure is more comprehensive than unit testing because of its
% % dependence on MCMC, which means a valid result requires correct
% % implementations of both the likelihood function and the proposal mechanism.

% % When analytical expectations are not available, the distribution
% % sampled from $I(M)$ with MCMC, $\text{P}_{\text{I}}(\theta)$, can be
% % compared to a target distribution sampled with $S(M)$
% % ($\text{P}_{\text{S}}(\theta)$, see box 2).
% % $S(M)$ can also be itself conveniently used by $I(M)$ as the proposal
% % density during MCMC (see listing 2 in the supplementary material for
% % an example).
% % This allows isolating the validation task to the model:
% % when other proposal densities are used, mismatched distributions can be
% % due to malfunctioning of either or both model and proposal mechanism.

% % \vspace{.5cm}
% % \noindent \emph{The model often recovers the right parameter values
% %   from data sets simulated at a (grid of) point(s) in parameter space}

% % Perhaps the most common approach to date for validating a
% % probabilistic model involves simulating data sets under specific parameter
% % values -- the scope here requires a simulator $S(M)$ used to simulate
% % \emph{data} under model $M$ -- carrying out inference, and tabulating
% % estimates depending on how close they were to the known, true values.
% % This procedure is particularly valuable in the context of point estimates
% % that ignore uncertainty, characteristic of maximum-likelihood methods (e.g.,
% % \citealp{mccormack09,han13,mendes17}), but of smaller if not unclear
% % utility in the Bayesian framework.

% % Differently from frequentist methods, which rely on approximate bootstrap
% % analyses, Bayesian models allow for exact statistical inference while
% % simultaneously addressing uncertainty.
% % As a result, parameter estimates come in the form of a posterior
% % distribution that depends not only on the model likelihood, but
% % also the priors of choice.
% % Validating a Bayesian model by trying to recover
% % hand-picked parameter values can be problematic because it is unclear
% % which prior is to be used, and also how often one should
% % expect a specific highest posterior density (HPD) interval to contain
% % the true parameter values.

% % Presumably, correct model implementations should yield HPDs that contain
% % the true values more often than not, but determining what ``often'' means
% % for complex models and priors is not trivial.
% % Thus, there are no theoretical guarantees that a model is correctly
% % implemented should most inferred 95\% HPDs include the true parameter
% % values, for example, or vice-versa.
% % LM: I thought so too, but it turns out that if you sample from the prior you *can* give guarantees.
% % This validation approach can still be useful in revealing regions
% % in parameter space where inference is easier or harder (given different
% % prior distributions), but we instead recommend procedures that are exact,
% % such as calibrated validation studies (see below).

% % \vspace{.5cm}
% % \noindent \emph{The model correctly estimates parameter values from
% %   data sets simulated from prior distributions, as indicated by the
% %   estimated posterior coverage}

% % Rather than simulating a fixed number of data sets for hand-picked
% % parameter values (see above), we can let the prior density of
% % different values dictate how often they are used in simulations.
% % This forms the basis of a calibrated validation study: a large number
% % of data sets are simulated directly from the prior distributions on
% % model parameters.
% % Here, the choice of prior is arbitrary, but ideally should reflect
% % what is currently known about the natural phenomenon being modeled.

% % The advantage of this approach is that if the same Bayesian model (which
% % includes likelihood \emph{and} priors) is used for both
% % simulation and inference, there are clear expectations under a
% % correctly implemented model.
% % Namely, out of a large number of simulations, the coverage of a
% % parameter should match the width of the HPD of choice
% % (coverage is defined as the number of times the HPD
% % interval contains the true value out of the total number of
% % simulations; \citealp{dawid82}).
% % For example, in the case of the commonly used 95\%-HPD interval, the true value
% % should often be inside the HPD between 90--98\% of the time (Table
% % \ref{tab:coverage2}).


% % Figure \ref{fig:yulecalval} shows the results of a calibrated
% % validation analysis for a simple Bayesian model.
% % Here, the full model is comprised of an exponential prior (with an
% % arbitrary mean of 0.01) on the birth-rate
% % $\lambda$, with the Yule likelihood (i.e., a pure-birth process)
% % modelling speciation times $\tau$ (Fig. \ref{fig:yulecalval}a).
% % We plot 100 ``true'' $\lambda$ values (drawn from the exponential prior)
% % against their posterior means estimated through MCMC from the
% % corresponding trees simulated under the Yule model
% % (Fig. \ref{fig:yulecalval}b-d).
% % Note that coverage is good for the first and second calibrated
% % validation analyses (the vast majority of 95\%-HPDs
% % include the true values, as shown by the blue vertical lines; Fig
% % \ref{fig:yulecalval}b-c), for which the model was correctly
% % specified.
% % Coverage is not good in the third analysis, where the model was
% % mispecified on purpose (a log-normal prior was used in inference,
% % rather than an exponential; Fig. \ref{fig:yulecalval}d).
% % Beyond coverage, one can also determine how good estimates can be,
% % potentially under several simulation scenarios, by investigating the
% % correlation between true parameter values and their posterior means.
% % Correlation should be high when the data carries a lot of
% % signal (Fig. \ref{fig:yulecalval}b), and low otherwise (Fig. \ref{fig:yulecalval}c).

% % Calibrated validation is a multifarious
% % analysis, as it not only verifies the correctness of a model
% % likelihood (both $S(M)$ and $I(M)$), but also of all the components
% % involved in the sampling of the posterior.
% % For this reason, calibrated validation is the most thorough,
% % integrated procedure one can carry out when the goal is to verify
% % implementation correctness.
% % We provide further guidelines and code examples for
% % calibrated validation in the supplementary material.

% % \subsection*{Validating the MCMC transition mechanism (operators)}

% % At the heart of MCMC as an approach to sample a target distribution
% % (conventionally, $\text{P}(\theta|D)$) is
% % the Metropolis-Hastings algorithm \citep{metropolis53,mh}.
% % This algorithm uses a transition mechanism (i.e., a set of
% % ``operators'') characterized by a proposal density $q(\theta'|\theta)$
% % that perturbs parameter values $\theta$ toward new values $\theta'$.
% % The Markov chain progresses as these perturbations are accepted, which
% % occurs with probability $\alpha$:

% % \begin{equation}
% %   \alpha = \text{min}\bigg(1, \frac{\text{P}(\theta'|D)}{\text{P}(\theta|D)} \frac{q(\theta|\theta')}{q(\theta'|\theta)} \bigg),
% % \end{equation}

% % \noindent where $\frac{q(\theta|\theta')}{q(\theta'|\theta)}$ is also
% % referred to as the Hastings ratio \citep{smith93,tierney94,gelman}.
% % For simple proposal densities, the Hastings ratio will often be unity and
% % only the prior and likelihood ratios must be computed.
% % It is sometimes the case, however, that more complex proposals will
% % increase or reduce the dimensionality of parameter space, in which
% % case the derivation of the Hastings ratio will be less straightforward.
% % For the sake of brevity, we point the reader to the relevant theory
% % and examples in \citep{green95,huelsenbeck04,drummond10}.

% % Although operators are not strictly part of the model (as mentioned
% % above, MCMC is not the only way to sample or approximate a target
% % distribution), it is absolutely vital to validate them prior or
% % together with the model per se, should MCMC be the approach of choice.
% % Only correctly implemented operators will lead to ergodic (i.e.,
% % irreducible, positive recurrent, and aperiodic) Markov
% % chains with a stationary distribution that will hopefully match
% % the target distribution, should the chain be long enough.

% % In the context of MCMC, unless a direct simulator $S(M)$ is
% % available to be used as a proposal mechanism (see above), model and operator
% % validation can be seen as two sides of the same coin.
% % Outside of unit-testing, validating models requires carrying out MCMC
% % (see items (ii) and (iv) in the previous section, for example), which
% % in turn can only be a meaningful procedure if correctly implemented
% % operators are available.
% % Conversely, if the intention is to validate new operators, then a
% % model under which to evaluate proposals must have been correctly
% % implemented prior to MCMC.

% % In the latter case, the principle behind validation is the same: one
% % compares the stationary distribution (with respect to one or more summary
% % statistics) obtained using the operator(s)
% % being tested, $\text{P}_{\text{I}}(\theta)$, with an expected
% % distribution, $\text{P}_{\text{S}}(\theta)$ (see listing 5 in
% % supplementary material). 
% % This second, expected distribution can be obtained through MCMC with a
% % mutually exclusive set of correctly implemented operators capable of
% % generating an ergodic chain (see item
% % (a) in supplementary table S1). 

% % Finally, a well-calibrated validation study also serves the purpose of
% % validating the transition mechanism.
% % Low coverage might indicate not only that the model was incorrectly
% % implemented, but also that operators are not functioning as expected.
% % Determining which of the two possibilities -- or whether both are
% % happening -- is not a trivial task, and careful diagnostics are
% % necessary on the part of method developers.

\subsection*{Software}

We implemented a suite of methods for automating many of the steps involved in coverage validation and RUV.
These methods were developed in Java and integrated into the BEAST 2 platform \citep{beast25}.
Code and a tutorial are available on \href{https://github.com/rbouckaert/DeveloperManual}{https://github.com/rbouckaert/DeveloperManual}.


\section*{Bayesian model validation guidelines for developers and reviewers}

In the previous sections, we described and executed two procedures for validating Bayesian models, namely, coverage validation and rank-uniformity validation (RUV).
Once both simulation and inference can be conducted under a model $\mathcal{M}$, executing these procedures amounts to following relatively straightforward protocols (Fig. \ref{fig:flowchart}).
Importantly, following these protocols should validate any Bayesian model, regardless of the nature of its parameter space and its component sampling distributions.
This is because such protocols provide clear, objective rules for
assessing model correctness, based on the coverage and rank
distribution of parameters values; both can be computed for any and all Bayesian models.
For the reasons above, carrying out an analysis of coverage and/or of the distribution of parameter-value ranks (with respect to their posterior samples) should on one hand be a requirement, and on another should suffice for introducing a new Bayesian model implementation.

In contrast, there is an infinite number of ways in which a new or published model can have its behavior inspected.
Researchers may want to know, given a model, how sensitive parameter estimates are to data set size, prior choice, model complexity, violation of model assumptions, to name a few.
Studies have examined how these factors affect estimation accuracy and precision (e.g., \citealp{zhang23,luo23}), as well as the mixing and convergence of MCMC chains (e.g., \citealp{nylander04,zhang23}).
We collectively refer to these examinations as ``model characterization'': any analysis of model behavior beyond assessing its correctness.
Model characterization is rarely carried out to satisfy the curiosity of the theoretician (but see, e.g., \citealp{tuffley97,steel20}); it is instead normally motivated by a model's empirical applications.
These investigations are thus critical for the longevity and popularity of a model, as domain experts will only adopt a model widely if they know when to trust the results and how to interpret them.

It is possible to characterize certain aspects of model behavior while simultaneously verifying its correctness, as discussed in the coverage validation section above.
For example, one can observe how accurate parameter estimates are (e.g., if the points in Fig. \ref{fig:yulecalval} fall on the identity line) under both correctly and incorrectly specified models.
However, the requirement of simulating parameter values from a prior distribution $f_{\Theta}(\cdot)$ during the validation of a model can complicate its characterization.
Depending on the characterization experiment's goals and design, researchers may find themselves rejecting a large fraction of simulated data sets -- perhaps because data sets do not resemble those in real life, or because they are too large to analyze.
But as we showed, rejecting draws in simulation may then be picked up by the validation protocol as an incorrectly implemented model.
This problem can only worsen the more dimensions of parameter space are allowed to vary.
In most cases, it may thus make more sense to first verify model correctness by following the procedures we described above, and then characterize model behavior further in a subsequent batch of analyses.

We conclude this section by proposing that scientists contributing or reviewing a new model ask the following question: Is the contribution at hand carrying out an empirical analysis that will specifically profit from scrutinizing model behavior?
If not, then model characterization efforts are not justified, and should take place at a different time and place, and be shouldered by the scientific community at large.


\section*{Concluding remarks}

In order to keep up with the large amounts of data of different kinds accumulating in public databases, researchers in the life sciences must constantly update their computational tool boxes.
New models are implemented in computational methods every day, but if they are not properly validated, downstream conclusions from using those methods will be void of any significance.
In the present study, we described and executed two distinct validation protocols that verify a Bayesian model has been correctly implemented.
Although we looked at examples from statistical phylogenetics, these two simulation-based protocols work for any and all Bayesian models.

We further elaborate on the difference between experiments in model validation versus model characterization.
The former are strictly concerned with theoretical expectations (e.g., about coverage) a model must meet if correctly implemented.
The latter, in contrast, include procedures more widely examining how model behavior responds to data set and model attributes; here, researchers often make predictions that are not theoretically guaranteed.
Some form of model validation should always be carried out when models are newly implemented, but the need for model characterization efforts is contextual.
Experiments probing model behavior beyond correctness are best designed and justified when empirically motivated.

We hope the guidelines described here can enhance both the release rate and standards of statistical software for biology, by assisting its users, developers and referees in quickly finding common ground when evaluating new modeling work.


\subsection*{Funding}
F.K.M. was supported by Marsden grant 16-UOA-277 and by the National Science Foundation (DEB-2040347).
R.B. was supported by Marsden grant \textcolor{orange}{[Fill out]}.
L.M.C was partially supported by the Coordena√ß√£o de Aperfei√ßoamento de
Pessoal de N√≠vel Superior - Brasil (CAPES) - Finance Code 001, and by the School of Applied Mathematics,  Getulio Vargas Foundation.
A.J.D. was supported by Marsden grant 16-UOA-277.

% % ----------------------------------------------------------------------------------------
% %	REFERENCE LIST
% % ----------------------------------------------------------------------------------------

% % \section*{References}
% % \clearpage

\bibliography{refs}

% % ----------------------------------------------------------------------------------------

% % \newpage
% % \appendix

\end{document}
